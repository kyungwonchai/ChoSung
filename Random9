import os
import sys

# ==========================================
# 프로젝트 설정 (경원님 환경에 맞춰 수정 가능)
# ==========================================
PROJECT_DIR = "smdlibllm"

# ==========================================
# 파일 생성기
# ==========================================
def create_file(path, content):
    full_path = os.path.join(PROJECT_DIR, path)
    os.makedirs(os.path.dirname(full_path), exist_ok=True)
    with open(full_path, "w", encoding="utf-8") as f:
        f.write(content.strip())
    print(f"[생성 완료] {full_path}")

def main():
    if os.path.exists(PROJECT_DIR):
        print(f"주의: '{PROJECT_DIR}' 폴더가 이미 존재합니다.")
    else:
        os.makedirs(PROJECT_DIR)

    # ---------------------------------------------------------
    # 1. requirements.txt (필요 라이브러리)
    # ---------------------------------------------------------
    create_file("requirements.txt", """
flask
requests
python-dotenv
pandas
""")

    # ---------------------------------------------------------
    # 2. .env (환경 변수 설정 - 프록시 및 API 키)
    # ---------------------------------------------------------
    create_file(".env", """
# [시스템 설정]
FLASK_PORT=5000
DEBUG_MODE=True

# [프록시 및 보안 설정]
# 사내망인 경우 아래 주소를 IT팀에 문의하여 기입 (http://proxy_address:port)
# 필요 없다면 비워두세요.
HTTP_PROXY=
HTTPS_PROXY=
# 사내 인증서가 필요한 경우 경로 지정 (예: ./certs/company-ca.pem), 없으면 False
REQUESTS_CA_BUNDLE=False

# [커스텀 LLM 설정]
# API 키 (모든 모델 공통 사용 시)
LLM_API_KEY=sk-smd-manufacturing-key-12345

# LLM 엔드포인트 리스트 (콤마로 구분하여 여러 개 입력 가능 - 로드밸런싱 용)
# 예: http://internal-ai-1.com/v1/chat/completions,http://internal-ai-2.com/v1/chat/completions
LLM_API_URLS=http://your-company-llm-server.com/v1/chat/completions

# 모델명 (사내 LLM이 요구하는 모델명)
LLM_MODEL_NAME=custom-model-v1
""")

    # ---------------------------------------------------------
    # 3. data/data.txt (샘플 데이터 - 엑셀 붙여넣기 형식)
    # ---------------------------------------------------------
    create_file("data/data.txt", """
PartNo\tSpec\tVendor\tThickness\tSize\tComment
2222-001424\t100nF 16V\tSamsung\t0.8mm\t1005\tStandard MLCC
2222-001425\t10uF 10V\tMurata\t1.2mm\t1608\tHigh Cap
3333-555555\tResistor 10k\tYageo\t0.4mm\t0603\tPrecision 1%
IC-74HC04\tHex Inverter\tTI\t2.5mm\tSOIC-14\tLogic Gate
""")

    # ---------------------------------------------------------
    # 4. src/__init__.py
    # ---------------------------------------------------------
    create_file("src/__init__.py", "")

    # ---------------------------------------------------------
    # 5. src/config.py (설정 로더)
    # ---------------------------------------------------------
    create_file("src/config.py", """
import os
from dotenv import load_dotenv

# .env 파일 로드
load_dotenv()

class Config:
    # Flask 설정
    PORT = int(os.getenv("FLASK_PORT", 5000))
    DEBUG = os.getenv("DEBUG_MODE", "False") == "True"

    # 프록시 설정 (Requests 라이브러리용 딕셔너리)
    PROXIES = {}
    if os.getenv("HTTP_PROXY"):
        PROXIES['http'] = os.getenv("HTTP_PROXY")
    if os.getenv("HTTPS_PROXY"):
        PROXIES['https'] = os.getenv("HTTPS_PROXY")

    # 인증서
    CA_BUNDLE = os.getenv("REQUESTS_CA_BUNDLE", "False")
    if CA_BUNDLE.lower() == "false":
        CA_BUNDLE = False

    # LLM 설정
    LLM_API_KEY = os.getenv("LLM_API_KEY", "")
    # 콤마로 구분된 URL을 리스트로 변환
    LLM_URLS = [url.strip() for url in os.getenv("LLM_API_URLS", "").split(',') if url.strip()]
    LLM_MODEL = os.getenv("LLM_MODEL_NAME", "default-model")
""")

    # ---------------------------------------------------------
    # 6. src/retriever.py (데이터 검색 엔진 - RAG 핵심)
    # ---------------------------------------------------------
    create_file("src/retriever.py", """
import os
import pandas as pd

class LocalRetriever:
    def __init__(self, data_path):
        self.data_path = data_path
        self.df = None
        self.load_data()

    def load_data(self):
        try:
            # 엑셀에서 복사하면 보통 탭(\\t)으로 구분됨. 스페이스면 sep='\\s+' 등으로 변경 필요
            # quotechar는 데이터 내 따옴표 무시 설정
            self.df = pd.read_csv(self.data_path, sep='\\t', dtype=str)
            self.df.fillna('', inplace=True) # NaN 제거
            print(f"[System] 데이터 로드 완료: {len(self.df)} 개의 부품 정보")
        except Exception as e:
            print(f"[Error] 데이터 로드 실패: {e}")
            self.df = pd.DataFrame()

    def search(self, query):
        if self.df.empty:
            return "데이터가 로드되지 않았습니다."

        results = []
        query = query.lower().strip()
        
        # 1. Exact Match: 부품 번호가 쿼리에 포함되어 있는지 확인 (가장 강력한 조건)
        # 데이터의 첫번째 컬럼(보통 PartNo)을 기준으로 검색
        key_col = self.df.columns[0]
        
        # 사용자가 질문에 부품번호를 섞어 쓰는 경우를 대비해 루프 탐색 (데이터가 수만건이면 비효율적일 수 있으나 내부망 서버에선 충분)
        matched_row = None
        
        # 단순 포함 여부 체크
        for index, row in self.df.iterrows():
            part_no = str(row[key_col]).lower()
            if part_no in query:
                matched_row = row
                break # 하나 찾으면 중단 (가장 긴 매칭을 원하면 로직 수정 필요)
        
        if matched_row is not None:
            # Row 데이터를 텍스트로 변환
            info_str = " | ".join([f"{col}: {val}" for col, val in matched_row.items()])
            return f"Found exact part match:\\n{info_str}"

        # 2. Keyword Search: 부품 번호를 못 찾았다면 전체 텍스트 검색 (최대 3개)
        # 간단한 검색 로직 구현
        mask = self.df.apply(lambda x: x.str.contains(query, case=False, na=False)).any(axis=1)
        filtered_df = self.df[mask].head(3)
        
        if not filtered_df.empty:
            docs = []
            for _, row in filtered_df.iterrows():
                docs.append(" | ".join([f"{col}: {val}" for col, val in row.items()]))
            return "Related parts found:\\n" + "\\n".join(docs)
            
        return "No specific part data found in the library."
""")

    # ---------------------------------------------------------
    # 7. src/llm_client.py (Curl 대체 - 다중 URL 로테이션)
    # ---------------------------------------------------------
    create_file("src/llm_client.py", """
import requests
import json
import itertools
from .config import Config

class CustomLLMClient:
    def __init__(self):
        # 여러 URL이 있을 경우 순서대로 돌아가며 사용 (Round Robin)
        self.url_cycle = itertools.cycle(Config.LLM_URLS) if Config.LLM_URLS else None

    def ask(self, context, question):
        if not self.url_cycle:
            return "LLM API URL이 설정되지 않았습니다. .env 파일을 확인해주세요."

        # RAG 프롬프트 구성
        system_prompt = (
            "You are a helpful assistant for SMD Manufacturing Process."
            "Answer the user's question based strictly on the provided Context below."
            "If the information is not in the Context, say you don't know."
            "Do not hallucinate technical specs."
        )
        
        user_message = f"Context:\\n{context}\\n\\nQuestion:\\n{question}"

        payload = {
            "model": Config.LLM_MODEL,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ],
            "temperature": 0.1 # 기술 데이터이므로 창의성 낮춤
        }

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {Config.LLM_API_KEY}"
        }

        # URL 로테이션 및 요청 시도
        # 여러 URL이 있다면 실패시 다음 URL로 재시도하는 로직을 추가할 수도 있음 (여기선 1회 시도 예시)
        target_url = next(self.url_cycle)
        
        try:
            print(f"[System] Requesting to LLM: {target_url}")
            response = requests.post(
                target_url, 
                headers=headers, 
                json=payload,
                proxies=Config.PROXIES,   # 프록시 적용
                verify=Config.CA_BUNDLE,  # SSL 인증서 검증 설정
                timeout=30 # 30초 타임아웃
            )
            response.raise_for_status()
            
            # 응답 파싱 (OpenAI 호환 포맷 가정)
            result = response.json()
            # 구조가 다를 경우 print(result)로 찍어보고 수정 필요
            return result['choices'][0]['message']['content']

        except Exception as e:
            return f"LLM 요청 중 에러 발생: {str(e)} (URL: {target_url})"
""")

    # ---------------------------------------------------------
    # 8. app.py (메인 Flask 어플리케이션)
    # ---------------------------------------------------------
    create_file("app.py", """
from flask import Flask, request, jsonify, render_template_string
from src.config import Config
from src.retriever import LocalRetriever
from src.llm_client import CustomLLMClient
import os

app = Flask(__name__)

# 데이터 파일 경로 (txt 파일)
DATA_FILE = os.path.join("data", "data.txt")

# 초기화
retriever = LocalRetriever(DATA_FILE)
llm_client = CustomLLMClient()

@app.route('/')
def home():
    # 간단한 테스트용 UI
    html = '''
    <!DOCTYPE html>
    <html>
    <head><title>SMD Library Agent</title></head>
    <body style="font-family: sans-serif; max-width: 800px; margin: 0 auto; padding: 20px;">
        <h2>SMD Component Query Agent</h2>
        <p>Enter a Part Number or question about specifications.</p>
        <div style="display: flex; gap: 10px;">
            <input type="text" id="query" style="flex-grow: 1; padding: 10px;" placeholder="Ex: 2222-001424 T값 뭐야?">
            <button onclick="ask()" style="padding: 10px 20px;">Ask</button>
        </div>
        <div id="loading" style="display:none; margin-top:20px;">Analyzing Data...</div>
        <div id="result" style="margin-top: 20px; white-space: pre-wrap; background: #f0f0f0; padding: 15px; border-radius: 5px; display:none;"></div>

        <script>
            async function ask() {
                const query = document.getElementById('query').value;
                document.getElementById('loading').style.display = 'block';
                document.getElementById('result').style.display = 'none';
                
                const response = await fetch('/api/chat', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({query: query})
                });
                
                const data = await response.json();
                document.getElementById('loading').style.display = 'none';
                document.getElementById('result').style.display = 'block';
                document.getElementById('result').innerText = data.answer || data.error;
            }
        </script>
    </body>
    </html>
    '''
    return render_template_string(html)

@app.route('/api/chat', methods=['POST'])
def chat():
    data = request.json
    user_query = data.get('query', '')
    
    if not user_query:
        return jsonify({"error": "No query provided"}), 400

    # 1. Retrieve (데이터 찾기)
    context_data = retriever.search(user_query)
    
    # 디버깅용 로그
    print(f"User Query: {user_query}")
    print(f"Retrieved Context: {context_data}")

    # 2. Generate (LLM 응답)
    # 검색된 데이터가 없으면 LLM에게 보내지 않고 바로 리턴할 수도 있음 (비용 절약)
    if "No specific part data found" in context_data and "Related parts" not in context_data:
        final_answer = "해당 부품 정보를 라이브러리 파일에서 찾을 수 없습니다."
    else:
        final_answer = llm_client.ask(context_data, user_query)

    return jsonify({
        "query": user_query,
        "context_used": context_data,
        "answer": final_answer
    })

if __name__ == '__main__':
    print(f"Server starting on port {Config.PORT}...")
    app.run(host='0.0.0.0', port=Config.PORT, debug=Config.DEBUG)
""")

    # ---------------------------------------------------------
    # 9. README.md (사용 설명서)
    # ---------------------------------------------------------
    create_file("README.md", """
# SMD Library Custom LLM Agent

SMD 제조 공정의 NPM 마운터 라이브러리 데이터(`data.txt`)를 기반으로 사용자의 질문에 답변하는 시스템입니다.

## 1. 초기 설정
1. `data/data.txt` 파일에 실제 엑셀 데이터를 붙여넣으세요. (탭으로 구분된 텍스트)
   - 첫 번째 줄은 헤더(PartNo, Spec 등)여야 합니다.
   - 첫 번째 컬럼은 반드시 부품 번호(Key)로 사용됩니다.
2. `.env` 파일을 열어 설정을 수정하세요.
   - `LLM_API_KEY`: 사내 LLM API 키
   - `LLM_API_URLS`: 사내 LLM 주소 (여러 개일 경우 콤마로 구분)
   - `HTTP_PROXY`: 프록시가 필요한 경우 설정

## 2. 실행 방법
```bash
# 1. 패키지 설치
pip install -r requirements.txt

# 2. 서버 실행
python app.py