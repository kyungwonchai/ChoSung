import os

# ==========================================
# [설정] 프로젝트 폴더명
# ==========================================
PROJECT_DIR = "smdlibllm_v2"

def create_file(path, content):
    full_path = os.path.join(PROJECT_DIR, path)
    os.makedirs(os.path.dirname(full_path), exist_ok=True)
    with open(full_path, "w", encoding="utf-8") as f:
        f.write(content.strip())
    print(f"[생성] {full_path}")

def main():
    if not os.path.exists(PROJECT_DIR):
        os.makedirs(PROJECT_DIR)

    # ---------------------------------------------------------
    # 1. requirements.txt
    # ---------------------------------------------------------
    create_file("requirements.txt", """
flask
requests
python-dotenv
pandas
markdown
""")

    # ---------------------------------------------------------
    # 2. .env (환경 변수)
    # ---------------------------------------------------------
    create_file(".env", """
FLASK_PORT=5000
DEBUG_MODE=True

# [프록시 설정]
HTTP_PROXY=
HTTPS_PROXY=
REQUESTS_CA_BUNDLE=False

# [커스텀 LLM API 설정]
LLM_API_KEY=your-secret-api-key
# 콤마(,)로 구분하여 여러 URL 입력 가능
LLM_API_URLS=http://your-custom-llm-url.com/run
""")

    # ---------------------------------------------------------
    # 3. config/persona.txt (페르소나/프롬프트 분리)
    # ---------------------------------------------------------
    create_file("config/persona.txt", """
[Role]
너는 SMD 제조 공정의 핵심 데이터를 관리하는 AI 어시스턴트다.
너는 제공된 데이터(Context)를 기반으로 엔지니어의 질문에 명확하고 간결하게 답변해야 한다.

[Data Schema - 엑셀 컬럼 정의]
- 1열: PartCode (부품 코드, Key)
- 2열: Spec (상세 스펙)
- 3열: Vendor (제조사)
- 4열: Thickness (부품 두께, 단위: mm)
- 5열: SizeCode (칩 사이즈 예: 1005, 1608)
- 6열: Remarks (비고)

[Output Format]
- 답변은 마크다운(Markdown) 형식을 사용하여 가독성을 높여라.
- 표(Table)가 필요하면 마크다운 표를 그려라.
- 부품의 상세 스펙을 나열할 때는 글머리 기호(Bullet points)를 사용해라.
- 사용자가 묻지 않은 사족(잡담)은 하지 마라.
""")

    # ---------------------------------------------------------
    # 4. data/data.txt (데이터 파일)
    # ---------------------------------------------------------
    create_file("data/data.txt", """
PartCode\tSpec\tVendor\tThickness\tSize\tRemarks
2222-001424\t100nF 16V\tSamsung\t0.8mm\t1005\tCommon usage
2222-001425\t10uF 10V\tMurata\t1.2mm\t1608\tHigh Temp
IC-74HC04\tHex Inverter\tTI\t2.5mm\tSOIC-14\tLogic
""")

    # ---------------------------------------------------------
    # 5. src/__init__.py & config.py
    # ---------------------------------------------------------
    create_file("src/__init__.py", "")
    create_file("src/config.py", """
import os
from dotenv import load_dotenv

load_dotenv()

class Config:
    PORT = int(os.getenv("FLASK_PORT", 5000))
    DEBUG = os.getenv("DEBUG_MODE", "False") == "True"
    
    PROXIES = {}
    if os.getenv("HTTP_PROXY"): PROXIES['http'] = os.getenv("HTTP_PROXY")
    if os.getenv("HTTPS_PROXY"): PROXIES['https'] = os.getenv("HTTPS_PROXY")
    
    CA_BUNDLE = os.getenv("REQUESTS_CA_BUNDLE", "False")
    if CA_BUNDLE.lower() == "false": CA_BUNDLE = False
    
    LLM_API_KEY = os.getenv("LLM_API_KEY", "")
    LLM_URLS = [url.strip() for url in os.getenv("LLM_API_URLS", "").split(',') if url.strip()]
    
    # 페르소나 파일 경로
    PERSONA_PATH = os.path.join(os.getcwd(), "config", "persona.txt")
""")

    # ---------------------------------------------------------
    # 6. src/retriever.py (검색기)
    # ---------------------------------------------------------
    create_file("src/retriever.py", """
import pandas as pd

class LocalRetriever:
    def __init__(self, data_path):
        self.data_path = data_path
        self.df = None
        self.load_data()

    def load_data(self):
        try:
            # 1열은 무조건 Key, 나머지는 데이터
            self.df = pd.read_csv(self.data_path, sep='\\t', header=None, dtype=str)
            self.df.fillna('', inplace=True)
            print(f"[System] 데이터 {len(self.df)}건 로드 완료")
        except Exception as e:
            print(f"[Error] 데이터 로드 실패: {e}")
            self.df = pd.DataFrame()

    def search(self, query):
        if self.df.empty: return ""
        query = query.strip().lower()
        
        found_rows = []
        
        # 1열(0번 인덱스) 매칭
        for idx, row in self.df.iterrows():
            part_key = str(row[0]).strip().lower()
            # 키가 쿼리에 포함되어 있거나, 쿼리가 키에 포함되어 있으면 (부분일치)
            if len(part_key) > 3 and part_key in query:
                found_rows.append(row)
        
        if not found_rows:
            return "라이브러리 파일에 해당 부품 코드가 없습니다."

        # 검색된 행들을 문자열로 변환 (헤더 정보가 없으므로 Col1, Col2... 형식이지만 페르소나에서 정의해줌)
        context_str = ""
        for row in found_rows:
            row_vals = " | ".join([str(val) for val in row.values])
            context_str += f"[{row[0]}] 데이터: {row_vals}\\n"
            
        return context_str
""")

    # ---------------------------------------------------------
    # 7. src/llm_client.py (페르소나 로드 및 호출)
    # ---------------------------------------------------------
    create_file("src/llm_client.py", """
import requests
import itertools
import os
from .config import Config

class CustomLLMClient:
    def __init__(self):
        self.url_cycle = itertools.cycle(Config.LLM_URLS) if Config.LLM_URLS else None

    def load_persona(self):
        try:
            with open(Config.PERSONA_PATH, 'r', encoding='utf-8') as f:
                return f.read()
        except:
            return "너는 도움이 되는 AI 어시스턴트다."

    def ask(self, context, user_question):
        if not self.url_cycle:
            return "오류: .env 설정 확인 필요"

        persona = self.load_persona()
        
        # 프롬프트 조립
        final_input = f"{persona}\\n\\n[참고 데이터(Context)]\\n{context}\\n\\n[사용자 질문]\\n{user_question}\\n\\n[지시]\\n위 참고 데이터를 바탕으로 답변해라."

        payload = {
            "input_type": "chat",
            "output_type": "chat",
            "input_value": final_input
        }

        headers = {
            "Content-Type": "application/json",
            "x-api-key": Config.LLM_API_KEY
        }

        target_url = next(self.url_cycle)

        try:
            response = requests.post(
                target_url, headers=headers, json=payload,
                proxies=Config.PROXIES, verify=Config.CA_BUNDLE, timeout=60
            )
            response.raise_for_status()
            
            # API 응답 처리 (JSON으로 온다고 가정)
            try:
                res_json = response.json()
                # 실제 LLM이 주는 키값(예: 'output_value', 'response', 'message' 등)에 맞춰 수정 필요할 수 있음
                # 여기선 전체 JSON을 문자열로 보거나, 딕셔너리라면 값을 추출 시도
                if isinstance(res_json, dict):
                    # 일반적인 키값들 순회하며 텍스트 찾기
                    for key in ['output_value', 'response', 'message', 'result', 'answer']:
                        if key in res_json:
                            return res_json[key]
                    return str(res_json) # 키를 못 찾으면 전체 리턴
                return str(res_json)
            except:
                return response.text

        except Exception as e:
            return f"**시스템 오류 발생:** {str(e)}"
""")

    # ---------------------------------------------------------
    # 8. templates/index.html (모던 다크모드 UI)
    # ---------------------------------------------------------
    create_file("templates/index.html", """
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SMD Library AI</title>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <style>
        :root {
            --bg-color: #343541;
            --chat-bg: #444654;
            --input-bg: #40414f;
            --text-color: #ececf1;
            --user-msg-bg: #343541;
            --bot-msg-bg: #444654;
            --border-color: #565869;
        }
        body {
            margin: 0; padding: 0;
            background-color: var(--bg-color);
            color: var(--text-color);
            font-family: 'Segoe UI', sans-serif;
            display: flex; flex-direction: column;
            height: 100vh;
        }
        #chat-container {
            flex: 1;
            overflow-y: auto;
            padding-bottom: 120px;
            scroll-behavior: smooth;
        }
        .message-row {
            padding: 24px 0;
            border-bottom: 1px solid rgba(0,0,0,0.1);
            display: flex;
            justify-content: center;
        }
        .message-row.user { background-color: var(--user-msg-bg); }
        .message-row.bot { background-color: var(--bot-msg-bg); border-bottom: 1px solid #2a2b32; }
        
        .message-content {
            width: 100%; max-width: 800px;
            padding: 0 20px;
            line-height: 1.6;
            font-size: 16px;
        }
        .sender-name {
            font-weight: bold; margin-bottom: 5px; font-size: 14px; opacity: 0.8;
        }
        .sender-name.user-name { color: #a3a3a3; }
        .sender-name.bot-name { color: #19c37d; }

        /* Markdown Styles */
        pre { background: #000; padding: 10px; border-radius: 5px; overflow-x: auto; }
        code { font-family: consolas, monospace; color: #e0e0e0; }
        table { border-collapse: collapse; width: 100%; margin: 10px 0; }
        th, td { border: 1px solid #666; padding: 8px; text-align: left; }
        th { background-color: #555; }

        /* Input Area */
        #input-area {
            position: fixed; bottom: 0; left: 0; right: 0;
            background: linear-gradient(180deg, rgba(53,53,65,0), #343541 20%);
            padding: 20px;
            display: flex; justify-content: center;
        }
        .input-box-wrapper {
            position: relative;
            width: 100%; max-width: 800px;
            background-color: var(--input-bg);
            border-radius: 12px;
            box-shadow: 0 0 15px rgba(0,0,0,0.2);
            border: 1px solid rgba(255,255,255,0.1);
            display: flex; flex-direction: column;
        }
        textarea {
            background: transparent; border: none; color: white;
            padding: 14px 45px 14px 14px;
            resize: none; outline: none;
            max-height: 200px; overflow-y: hidden;
            font-family: inherit; font-size: 16px; line-height: 24px;
        }
        button {
            position: absolute; bottom: 8px; right: 10px;
            background: #19c37d; border: none; border-radius: 4px;
            color: white; padding: 6px 12px; cursor: pointer;
            transition: background 0.2s;
        }
        button:hover { background: #1a8f61; }
        button:disabled { background: #555; cursor: not-allowed; }
        
        /* Loading Animation */
        .typing-indicator span {
            display: inline-block; width: 6px; height: 6px;
            background-color: #ccc; border-radius: 50%;
            animation: typing 1s infinite; margin: 0 2px;
        }
        @keyframes typing { 0%, 100% { opacity: 0.2; } 50% { opacity: 1; } }
    </style>
</head>
<body>

<div id="chat-container">
    <div class="message-row bot">
        <div class="message-content">
            <div class="sender-name bot-name">SMD AI Agent</div>
            <div class="markdown-body">
                안녕하세요! SMD 부품 라이브러리에 대해 무엇이든 물어보세요.<br>
                (예: "2222-001424 스펙이 뭐야?", "IC-74HC04 두께 알려줘")
            </div>
        </div>
    </div>
</div>

<div id="input-area">
    <div class="input-box-wrapper">
        <textarea id="user-input" rows="1" placeholder="질문을 입력하세요... (Shift+Enter 줄바꿈)"></textarea>
        <button id="send-btn" onclick="sendMessage()">Send</button>
    </div>
</div>

<script>
    const chatContainer = document.getElementById('chat-container');
    const userInput = document.getElementById('user-input');
    const sendBtn = document.getElementById('send-btn');

    // Auto-resize textarea
    userInput.addEventListener('input', function() {
        this.style.height = 'auto';
        this.style.height = (this.scrollHeight) + 'px';
        if(this.value === '') this.style.height = 'auto';
    });

    // Handle Enter key
    userInput.addEventListener('keydown', function(e) {
        if (e.key === 'Enter' && !e.shiftKey) {
            e.preventDefault();
            sendMessage();
        }
    });

    async function sendMessage() {
        const text = userInput.value.trim();
        if (!text) return;

        // Add User Message
        appendMessage('User', text, 'user');
        userInput.value = '';
        userInput.style.height = 'auto';
        sendBtn.disabled = true;

        // Add Loading Indicator
        const loadingId = appendLoading();

        try {
            const response = await fetch('/chat', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ query: text })
            });
            const data = await response.json();
            
            // Remove Loading & Add Bot Message
            document.getElementById(loadingId).remove();
            appendMessage('SMD AI', data.answer, 'bot');
        } catch (err) {
            document.getElementById(loadingId).remove();
            appendMessage('System', '오류가 발생했습니다: ' + err, 'bot');
        } finally {
            sendBtn.disabled = false;
        }
    }

    function appendMessage(sender, text, type) {
        const div = document.createElement('div');
        div.className = `message-row ${type}`;
        
        let contentHtml = '';
        if (type === 'bot' && typeof marked !== 'undefined') {
            contentHtml = marked.parse(text); // Render Markdown
        } else {
            contentHtml = text.replace(/\\n/g, '<br>');
        }

        div.innerHTML = `
            <div class="message-content">
                <div class="sender-name ${type}-name">${sender}</div>
                <div class="markdown-body">${contentHtml}</div>
            </div>
        `;
        chatContainer.appendChild(div);
        chatContainer.scrollTop = chatContainer.scrollHeight;
    }

    function appendLoading() {
        const id = 'loading-' + Date.now();
        const div = document.createElement('div');
        div.id = id;
        div.className = 'message-row bot';
        div.innerHTML = `
            <div class="message-content">
                <div class="sender-name bot-name">SMD AI</div>
                <div class="typing-indicator"><span></span><span></span><span></span></div>
            </div>
        `;
        chatContainer.appendChild(div);
        chatContainer.scrollTop = chatContainer.scrollHeight;
        return id;
    }
</script>

</body>
</html>
""")

    # ---------------------------------------------------------
    # 9. app.py (라우팅)
    # ---------------------------------------------------------
    create_file("app.py", """
from flask import Flask, request, jsonify, render_template
from src.config import Config
from src.retriever import LocalRetriever
from src.llm_client import CustomLLMClient
import os

app = Flask(__name__)

# 경로 설정
DATA_FILE = os.path.join("data", "data.txt")
TEMPLATE_DIR = os.path.join(os.getcwd(), "templates")
app.template_folder = TEMPLATE_DIR

retriever = LocalRetriever(DATA_FILE)
llm_client = CustomLLMClient()

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/chat', methods=['POST'])
def chat():
    data = request.json
    query = data.get('query', '')
    
    # 1. 데이터 검색
    context = retriever.search(query)
    
    # 2. LLM 호출
    if "없습니다" in context:
        # 데이터가 없어도 페르소나를 통해 정중히 거절하도록 LLM에 넘길 수도, 바로 리턴할 수도 있음
        # 여기선 정확도를 위해 LLM에게 '데이터 없음' 상황을 전달
        answer = llm_client.ask("해당 부품 데이터가 라이브러리에 없음.", query)
    else:
        answer = llm_client.ask(context, query)
        
    return jsonify({"answer": answer})

if __name__ == '__main__':
    print(f"Server running at http://0.0.0.0:{Config.PORT}")
    app.run(host='0.0.0.0', port=Config.PORT, debug=Config.DEBUG)
""")

    print("\\n[완료] 'smdlibllm_v2' 폴더 생성 완료.")
    print("-----------------------------------------------------")
    print("1. cd smdlibllm_v2")
    print("2. pip install -r requirements.txt")
    print("3. .env 파일 수정 (LLM_API_URLS, LLM_API_KEY 설정)")
    print("4. config/persona.txt 수정 (엑셀 컬럼 정의, 지시사항)")
    print("5. python app.py 실행 -> http://localhost:5000 접속")
    print("-----------------------------------------------------")

if __name__ == "__main__":
    main()