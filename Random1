네, 맞습니다. 회사 방화벽(보안 정책) 때문에 코드 실행 중에 허깅페이스 사이트에서 모델을 자동으로 다운로드하지 못하는 것이 거의 확실합니다.

이 문제를 해결하기 위해 모델을 인터넷이 되는 곳에서 직접 다운로드한 후, 로컬에 저장된 그 파일을 사용하도록 코드를 수정해야 합니다. 설정 변경이나 방화벽 해제 요청보다 이 방법이 훨씬 간단하고 빠릅니다.

아래에 1) 모델 파일 다운로드 방법과 2) 로컬 파일을 사용하도록 수정한 전체 코드를 상세히 설명해 드리겠습니다.

## 1. 허깅페이스 사이트에서 모델 파일 직접 다운로드하기
인터넷 접속이 가능한 환경(집 컴퓨터 등)에서 아래 두 모델의 파일을 다운로드하여 USB나 다른 방법으로 Jupyer를 실행하는 환경으로 옮겨야 합니다.

1.1. 임베딩 모델 다운로드 (BM-K/KoSimCSE-roberta-multitask)
폴더 생성: 모델 파일을 담을 폴더를 미리 만듭니다.

예시: C:/my_local_models/embedding_model

다운로드 (Git 사용이 가장 편리):

PC에 Git이 설치되어 있어야 합니다.

명령 프롬프트(cmd)나 터미널을 열고 아래 명령어를 순서대로 실행합니다.

Bash

# Git LFS(대용량 파일 시스템) 설치 (처음 한 번만)
git lfs install

# 1번에서 만든 폴더로 이동
cd C:/my_local_models/embedding_model

# 허깅페이스 저장소 복제(clone)
git clone https://huggingface.co/BM-K/KoSimCSE-roberta-multitask .
마지막 명령어 끝에 .(점)을 꼭 찍어주세요. 현재 폴더에 파일들이 다운로드됩니다.

1.2. LLM(답변 생성) 모델 다운로드 (yanado/KORani-v2-1.3B)
폴더 생성: LLM 모델을 담을 별도의 폴더를 만듭니다.

예시: C:/my_local_models/llm_model

다운로드 (Git 사용):

명령 프롬프트(cmd)를 열고 아래 명령어를 실행합니다.

Bash

# 1번에서 만든 폴더로 이동
cd C:/my_local_models/llm_model

# 허깅페이스 저장소 복제(clone)
git clone https://huggingface.co/yanado/KORani-v2-1.3B .
이제 C:/my_local_models 폴더 안에 두 모델의 모든 파일이 저장되었습니다.

## 2. 로컬 모델을 사용하도록 수정한 전체 Python 코드
이제 파이썬 코드에서 인터넷 주소 대신 방금 다운로드한 로컬 폴더 경로를 알려주도록 수정합니다.

수정된 부분은 ### --- 로컬 모델 로드 --- ### 주석으로 표시했습니다.

Python

import pandas as pd
import psycopg2
from psycopg2.extras import RealDictCursor
from qdrant_client import QdrantClient, models
from sentence_transformers import SentenceTransformer
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch
import re

# --- 1. DB에서 데이터 로드 (이전과 동일) ---
def load_data_from_db(conn_info: dict) -> pd.DataFrame:
    try:
        print("데이터베이스에 연결합니다...")
        conn = psycopg2.connect(**conn_info)
        cursor = conn.cursor(cursor_factory=RealDictCursor)
        query = "SELECT * FROM parts_data;"
        cursor.execute(query)
        rows = cursor.fetchall()
        cursor.close()
        conn.close()
        print(f"{len(rows)}개의 데이터를 DB에서 성공적으로 불러왔습니다.")
        return pd.DataFrame(rows)
    except Exception as e:
        print(f"데이터베이스 연결 오류: {e}")
        return pd.DataFrame()

# --- 2. 로컬 모델 로드 --- ###
# ?? 수정된 부분 1: 인터넷 주소 대신 다운로드한 로컬 폴더 경로를 지정합니다.
embedding_model_path = "C:/my_local_models/embedding_model"
llm_model_path = "C:/my_local_models/llm_model"

print(f"로컬 경로에서 임베딩 모델을 로드합니다: {embedding_model_path}")
embedding_model = SentenceTransformer(embedding_model_path)
print("임베딩 모델 로드 완료.")

# Qdrant 준비
qdrant_client = QdrantClient(":memory:")
qdrant_client.recreate_collection(
    collection_name="parts_db_collection",
    vectors_config=models.VectorParams(
        size=embedding_model.get_sentence_embedding_dimension(),
        distance=models.Distance.COSINE
    )
)

# --- 3. 데이터 주입 (이전과 동일) ---
db_connection_info = {
    "host": "YOUR_DB_HOST", "dbname": "YOUR_DB_NAME",
    "user": "YOUR_DB_USER", "password": "YOUR_DB_PASSWORD"
}
parts_df = load_data_from_db(db_connection_info)

if not parts_df.empty:
    points_to_ingest = []
    for idx, row in parts_df.iterrows():
        text_to_embed = f"부품명: {row.get('part_name', '')}, 상태: {row.get('status_msg', '')}, 타입: {row.get('type', '')}"
        vector = embedding_model.encode(text_to_embed).tolist()
        payload = row.to_dict()
        points_to_ingest.append(models.PointStruct(id=idx, vector=vector, payload=payload))
    qdrant_client.upsert(collection_name="parts_db_collection", points=points_to_ingest, wait=True)
    print("DB 데이터를 Qdrant에 성공적으로 주입했습니다.")

# --- 4. 로컬 LLM 로드 및 하이브리드 함수 --- ###
print(f"로컬 경로에서 LLM을 로드합니다: {llm_model_path}")
device = "cuda" if torch.cuda.is_available() else "cpu"

# ?? 수정된 부분 2: 여기도 로컬 폴더 경로를 사용합니다.
tokenizer = AutoTokenizer.from_pretrained(llm_model_path)
model = AutoModelForCausalLM.from_pretrained(llm_model_path).to(device)

generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)
print("LLM 로드 완료.")

def ask_hybrid_system(question: str):
    part_code_pattern = r'[A-Z0-9]{4}-[A-Z0-9]{5,}'
    found_codes = re.findall(part_code_pattern, question)
    
    if found_codes:
        part_code = found_codes[0]
        print(f"부품 코드 '{part_code}' 직접 조회...")
        hits = qdrant_client.scroll(
            collection_name="parts_db_collection",
            scroll_filter=models.Filter(must=[models.FieldCondition(key="part_name", match=models.MatchValue(value=part_code))]),
            limit=1
        )
        return f"'{part_code}' 부품 정보:\n{hits[0][0].payload}" if hits[0] else f"'{part_code}' 정보를 찾을 수 없습니다."
    else:
        print("자연어 질문 RAG 검색...")
        question_vector = embedding_model.encode(question).tolist()
        hits = qdrant_client.search(collection_name="parts_db_collection", query_vector=question_vector, limit=1)
        if not hits: return "관련 정보를 찾을 수 없습니다."
        
        context = str(hits[0].payload)
        prompt = f"### 지시:\n주어진 '정보'를 바탕으로 '질문'에 대해 한국어로 간결하게 답변하세요. 정보에 없는 내용은 절대로 언급하지 마세요.\n\n### 정보:\n{context}\n\n### 질문:\n{question}\n\n### 답변:\n"
        response = generator(prompt, max_new_tokens=150, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)
        answer = response[0]['generated_text'].replace(prompt, "").strip()
        return answer

# --- 5. 테스트 (이전과 동일) ---
print("\n--- RAG 시스템 테스트 시작 ---")
print(ask_hybrid_system("6HS2-00978A 부품의 전체 정보가 궁금해."))
print("\n---")
print(ask_hybrid_system("Parts Empty Stop을 사용하고 Set count가 Yes인 부품은 뭐야?"))
이제 이 코드를 실행하면 인터넷에 접속하는 대신, 지정된 로컬 폴더에서 모델을 직접 불러오기 때문에 회사 방화벽에 막히지 않고 정상적으로 작동할 것입니다.