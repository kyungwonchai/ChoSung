네, 알겠습니다. 지금까지 논의한 모든 기능(API 기반, 수동/AI 개입, 협업, 로깅)을 포함하는 **'지능형 작업 큐 시스템'**을 우분투 24.04 LTS 서버 1대에 Docker Compose를 사용하여 실제로 구축하는 전체 과정을 A to Z로, 매우 상세하게 안내해 드리겠습니다.

🚀 최종 아키텍처
우리는 '공유 주방' 비유처럼, 하나의 우분투 서버(주방 건물) 안에 기능별로 분리된 컨테이너(전문 식당)들을 Docker Compose(설계도)를 이용해 구축할 것입니다.

[시스템 구성도]

[사용자 PC / 로봇 / AI]  <-- (HTTP 요청) -->  [ ⓵ API 서버 (FastAPI) ]
                                                |
                               +----------------+----------------+
                               |                |                |
                       [ ⓶ Redis ]      [ ⓷ PostgreSQL ]      [ ... ]
                       (작업 큐)        (작업로그/DB)
## ⚙️ 1단계: 프로젝트 환경 설정
가장 먼저, 프로젝트를 위한 폴더와 기본 설계도(docker-compose.yml)를 준비합니다.

1. 프로젝트 폴더 생성
우분투 서버에 SSH로 접속하여 아래 명령어를 실행합니다.

Bash

# 프로젝트를 위한 폴더를 만들고 그 안으로 이동합니다.
mkdir intelligent_queue && cd intelligent_queue

# API 서버의 소스 코드를 담을 폴더를 미리 만듭니다.
mkdir api
2. docker-compose.yml 작성 (시스템 전체 설계도)
intelligent_queue 폴더 안에 docker-compose.yml 파일을 생성하고 아래 내용을 작성합니다. 이 파일이 우리 시스템의 모든 구성 요소를 정의하고 연결하는 심장부입니다.

YAML

version: '3.8'

services:
  # 1. API 서버: 모든 요청을 처리하는 관문
  api_server:
    container_name: api_server
    # ./api 폴더에 있는 Dockerfile을 사용하여 이미지를 빌드
    build: ./api
    # 외부 8000 포트를 컨테이너 8000 포트로 연결
    ports:
      - "8000:8000"
    # 소스 코드 변경 시 자동으로 재시작되도록 볼륨 설정 (개발 편의성)
    volumes:
      - ./api:/app
    # 환경 변수 설정 (DB, Redis 접속 정보)
    environment:
      - DATABASE_URL=postgresql://user:password@database:5432/mydatabase
      - REDIS_HOST=message_queue
    # DB와 Redis가 먼저 실행된 후에 API 서버가 실행되도록 의존성 설정
    depends_on:
      - database
      - message_queue

  # 2. 메시지 큐: 작업 대기열을 관리
  message_queue:
    container_name: message_queue
    image: "redis:7-alpine"
    ports:
      - "6379:6379"

  # 3. 데이터베이스: 모든 로그와 영구 데이터를 저장
  database:
    container_name: database
    image: "postgres:15-alpine"
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=mydatabase
    volumes:
      # 컨테이너가 삭제돼도 데이터는 보존되도록 볼륨 연결
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

volumes:
  postgres_data:
## 🐍 2단계: API 서버 개발 (FastAPI)
이제 시스템의 두뇌 역할을 할 API 서버의 코드를 작성합니다.

1. api/requirements.txt 작성 (필요한 파이썬 라이브러리)
api 폴더 안에 필요한 라이브러리 목록 파일을 만듭니다.

Plaintext

fastapi
uvicorn[standard]
redis
psycopg2-binary
sqlalchemy
2. api/main.py 작성 (API 핵심 로직)
api 폴더 안에 API의 모든 기능을 담은 메인 코드를 작성합니다.

Python

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import redis
import psycopg2
from psycopg2.extras import RealDictCursor
import json
import os
from datetime import datetime

# --- 설정 ---
REDIS_HOST = os.getenv("REDIS_HOST", "localhost")
DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://user:password@localhost/mydatabase")

# --- FastAPI 앱 초기화 ---
app = FastAPI(title="지능형 작업 큐 시스템 API")

# --- Redis 및 DB 연결 ---
# Redis에서는 'wait_queue'라는 리스트를 작업 큐로 사용합니다.
r = redis.Redis(host=REDIS_HOST, port=6379, db=0, decode_responses=True)
db_conn = psycopg2.connect(DATABASE_URL)

# --- 데이터 모델 정의 (Pydantic) ---
class Job(BaseModel):
    job_id: str
    description: str
    priority: int = 10 # 우선순위 (낮을수록 높음)
    details: dict

# --- API 엔드포인트 구현 ---

@app.on_event("startup")
async def startup_event():
    # 프로그램 시작 시 DB에 로그 테이블 생성
    with db_conn.cursor() as cursor:
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS job_logs (
                id SERIAL PRIMARY KEY,
                timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                actor_id VARCHAR(100),
                action VARCHAR(50),
                job_id VARCHAR(100),
                reason TEXT,
                details JSONB
            );
        """)
        db_conn.commit()

def log_action(actor_id, action, job_id, reason, details={}):
    """모든 변경 사항을 DB에 기록하는 함수"""
    with db_conn.cursor() as cursor:
        cursor.execute(
            "INSERT INTO job_logs (actor_id, action, job_id, reason, details) VALUES (%s, %s, %s, %s, %s)",
            (actor_id, action, job_id, reason, json.dumps(details))
        )
        db_conn.commit()

@app.post("/jobs", summary="신규 작업 추가")
async def add_job(job: Job, actor_id: str = "System"):
    job_json = job.model_dump_json()
    # lpush: 큐의 맨 앞에 작업을 추가 (우선순위는 나중에 정렬 로직에서 사용)
    r.lpush("wait_queue", job_json)
    log_action(actor_id, "ADD_JOB", job.job_id, "신규 작업 등록")
    return {"message": "Job added successfully", "job": job}

@app.get("/jobs", summary="현재 대기 큐 조회")
async def get_jobs():
    queue_json = r.lrange("wait_queue", 0, -1)
    queue = [json.loads(j) for j in queue_json]
    return {"queue": queue}

@app.post("/jobs/{job_id}/claim", summary="작업 가져가기 (수동 처리)")
async def claim_job(job_id: str, actor_id: str, reason: str):
    # 실제 구현에서는 job_id를 찾아 정확히 제거하는 로직이 필요
    # 여기서는 간단히 맨 뒤 작업을 가져간다고 가정
    job_json = r.rpop("wait_queue")
    if not job_json:
        raise HTTPException(status_code=404, detail="No jobs in queue")
    
    job_data = json.loads(job_json)
    log_action(actor_id, "CLAIM_JOB", job_data['job_id'], reason, details=job_data)
    return {"message": f"Job {job_data['job_id']} claimed by {actor_id}", "claimed_job": job_data}

# TODO: AI 재정렬, 우선순위 수정 등의 API 추가 ...
3. api/Dockerfile 작성 (API 서버 설계도)
마지막으로 api 폴더에 API 서버를 컨테이너로 만드는 설계도를 작성합니다.

Dockerfile

FROM python:3.9-slim

WORKDIR /app

# 라이브러리 설치
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# 소스코드 복사
COPY . .

# FastAPI 서버 실행
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
## 🎬 3단계: 시스템 전체 실행 및 테스트
이제 모든 준비가 끝났습니다. 명령어 한 줄로 시스템 전체를 실행하고 테스트해 봅니다.

1. 시스템 시작
intelligent_queue 폴더(최상위 폴더)에서 아래 명령어를 실행합니다.

Bash

# --build 옵션으로 이미지를 새로 빌드하며 모든 컨테이너를 백그라운드(-d)로 실행
docker-compose up --build -d
docker-compose ps 명령어로 3개의 컨테이너(api_server, message_queue, database)가 모두 Up 상태인지 확인합니다.

2. API 테스트 (터미널에서 curl 사용)
(A) 신규 작업 2개 추가:

Bash

curl -X POST "http://localhost:8000/jobs?actor_id=Manager_Lee" -H "Content-Type: application/json" -d '{"job_id": "TASK-001", "description": "B구역 자재 준비", "priority": 5, "details": {"material": "2222-008888"}}'
curl -X POST "http://localhost:8000/jobs?actor_id=Robot_01" -H "Content-Type: application/json" -d '{"job_id": "TASK-002", "description": "C구역 피더 교체", "priority": 10, "details": {"feeder": "15R"}}'
(B) 현재 큐 상태 확인:

Bash

curl http://localhost:8000/jobs
방금 추가한 2개의 작업이 큐에 들어있는 것을 JSON 형태로 볼 수 있습니다.

(C) PC-B 사용자가 수동으로 작업 가져가기:

Bash

curl -X POST "http://localhost:8000/jobs/TASK-002/claim?actor_id=PC-B&reason=긴급라인정지요청"
(D) 다시 큐 상태 확인:

Bash

curl http://localhost:8000/jobs
작업 하나가 큐에서 사라진 것을 확인할 수 있습니다. 데이터베이스의 job_logs 테이블에는 'PC-B'가 '긴급라인정지요청' 사유로 작업을 가져갔다는 로그가 기록되어 있을 것입니다.