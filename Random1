import pdfplumber
import re
import statistics
from typing import List, Dict, Any
from collections import defaultdict
import numpy as np

# ==============================================================================
# 데이터 구조 클래스 (변경 없음)
# ==============================================================================
class TextElement:
    def __init__(self, element: Dict[str, Any]):
        self.value = element.get('text', '')
        self.x0, self.y0, self.x1, self.y1 = [round(element.get(k, 0), 2) for k in ['x0', 'y0', 'x1', 'y1']]
        self.size = round(element.get('size', 0), 2)
        self.color = element.get('non_stroking_color', (0, 0, 0))

class PartComponent:
    def __init__(self, page_number: int, part_number_element: TextElement):
        self.page_number = page_number
        self.part_number = part_number_element
        self.related_elements: List[TextElement] = []

    def add_element(self, element: TextElement):
        self.related_elements.append(element)

    def get_details_string(self) -> str:
        if not self.related_elements: return "No other details found."
        sorted_elements = sorted(self.related_elements, key=lambda e: (e.y0, e.x0))
        details_list = [f"Color: {e.color}, Size: {e.size}, Value: '{e.value}', Position: (x0={e.x0}, y0={e.y0}, x1={e.x1}, y1={e.y1})" for e in sorted_elements]
        return ",\n\t".join(details_list)

# ==============================================================================
# ? 콘텐츠 자동 탐지 기능이 적용된 메인 분석 함수 ?
# ==============================================================================
def analyze_parts_from_pdf(pdf_path: str, output_txt_path: str):
    all_part_components = []
    part_number_pattern = re.compile(r"^\d{4}-\d{6}$")

    print(f"PDF 분석 시작: {pdf_path}")
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages, 1):
                print(f"\n--- 페이지 {page_num} 처리 시작 ---")
                
                # 고정된 crop 대신, 페이지 전체의 단어를 일단 추출
                raw_words = page.extract_words(extra_attrs=["size", "non_stroking_color"])
                
                if not raw_words:
                    print(f"페이지 {page_num}에서 단어를 찾을 수 없습니다.")
                    continue

                # 1. ? 먼저 페이지 전체에서 부품 번호를 모두 찾습니다.
                part_number_objects = [w for w in raw_words if part_number_pattern.match(w.get('text', ''))]

                if not part_number_objects:
                    print(f"페이지 {page_num}에서 부품 번호(xxxx-xxxxxx)를 찾지 못해 건너뜁니다.")
                    continue
                
                # 2. ? 찾은 부품 번호들의 위치를 기반으로 '콘텐츠 영역'을 동적으로 설정합니다.
                content_top = min(p['top'] for p in part_number_objects)
                content_bottom = max(p['bottom'] for p in part_number_objects)
                # 약간의 여유(padding)를 줍니다.
                content_top -= 20
                content_bottom += 20
                
                print(f"부품 번호 위치 기반, 콘텐츠 영역 y:[{content_top:.2f} ~ {content_bottom:.2f}] 자동 설정")

                # 3. ? '콘텐츠 영역' 내의 유효한 단어들만 필터링합니다.
                valid_words = [
                    w for w in raw_words
                    if content_top <= w.get('top', 0) and w.get('bottom', 0) <= content_bottom
                ]

                if not valid_words:
                    print(f"자동 설정된 콘텐츠 영역 내에서 유효한 텍스트를 찾지 못했습니다.")
                    continue
                
                print(f"콘텐츠 영역 내 {len(valid_words)}개의 유효 단어로 분석을 진행합니다.")
                
                sorted_words = sorted(valid_words, key=lambda w: w['y0'])

                # 4. ? 통계적 분석으로 그룹핑을 수행합니다 (이전 로직과 동일).
                gaps = [
                    sorted_words[i+1]['y0'] - sorted_words[i]['y1']
                    for i in range(len(sorted_words) - 1)
                    if sorted_words[i+1]['y0'] > sorted_words[i]['y1']
                ]
                
                if not gaps:
                    blocks = [sorted_words]
                else:
                    q1, q3 = np.percentile(gaps, [25, 75])
                    iqr = q3 - q1
                    separation_threshold = q3 + 1.5 * iqr
                    blocks, current_block = [], [sorted_words[0]]
                    for i in range(len(sorted_words) - 1):
                        gap = sorted_words[i+1]['y0'] - sorted_words[i]['y1']
                        if gap > separation_threshold:
                            blocks.append(current_block)
                            current_block = [sorted_words[i+1]]
                        else:
                            current_block.append(sorted_words[i+1])
                    blocks.append(current_block)
                
                print(f"페이지 {page_num}에서 {len(blocks)}개의 정보 블록을 감지했습니다.")

                # 5. 분리된 각 블록을 처리합니다.
                for block in blocks:
                    found_part_number_obj = next((w for w in block if part_number_pattern.match(w.get('text', ''))), None)
                    if found_part_number_obj:
                        part_element = TextElement(found_part_number_obj)
                        component = PartComponent(page_num, part_element)
                        for word_obj in block:
                            if word_obj is not found_part_number_obj:
                                component.add_element(TextElement(word_obj))
                        all_part_components.append(component)

        # 6. 최종 결과를 페이지별로 출력합니다.
        page_map = defaultdict(list)
        for component in all_part_components:
            page_map[component.page_number].append(component)

        with open(output_txt_path, 'w', encoding='utf-8') as f:
            f.write(f"총 {len(all_part_components)}개의 부품을 {len(page_map)}개 페이지에서 찾았습니다.\n")
            for page_num in sorted(page_map.keys()):
                components_on_page = page_map[page_num]
                f.write("\n" + "="*25 + f" PAGE {page_num} " + "="*25 + "\n")
                f.write(f"({len(components_on_page)}개의 부품 발견)\n\n")
                for i, component in enumerate(components_on_page, 1):
                    f.write(f"--- {i}. PartNumber: {component.part_number.value} ---\n")
                    f.write(f"Details: [\n\t{component.get_details_string()}\n]\n")
                    f.write("-" * 60 + "\n")

        print(f"\n분석 완료! 결과가 '{output_txt_path}' 파일에 저장되었습니다.")

    except Exception as e:
        print(f"스크립트 실행 중 오류가 발생했습니다: {e}")
        import traceback
        traceback.print_exc()

# --- 메인 실행 부분 ---
if __name__ == "__main__":
    # numpy 라이브러리가 필요합니다: pip install numpy
    pdf_file_path = "YOUR_PDF_FILE_PATH.pdf" 
    output_file_path = "part_list_final_output.txt"
    analyze_parts_from_pdf(pdf_file_path, output_file_path)