Hadoop)은 대용량 데이터 처리를 위한 오픈소스 프레임워크입니다. 하둡의 주요 목적은 분산 컴퓨팅을 통해 빅 데이터를 빠르고 효율적으로 처리하는 것입니다. 여기에는 주로 다음과 같은 컴포넌트들이 포함됩니다.
핵심 컴포넌트:

    HDFS(Hadoop Distributed File System): 분산 파일 시스템으로, 데이터를 여러 노드에 분산 저장합니다. HDFS는 네임노드(NameNode)와 데이터노드(DataNode)로 구성됩니다. 네임노드가 메타데이터를 관리하고, 데이터노드가 실제 데이터를 저장합니다.

    MapReduce: 데이터 처리를 위한 프로그래밍 모델입니다. 작업을 'Map'과 'Reduce' 두 단계로 분리하여 병렬 처리를 수행합니다. 각 노드에서 독립적으로 작업을 수행한 후, 결과를 집계합니다.

    YARN(Yet Another Resource Negotiator): 하둡 2.x 버전에서 도입된 클러스터 리소스 관리 시스템입니다. CPU, 메모리 등의 리소스를 효율적으로 분배하고 관리합니다.

추가 컴포넌트:

    Hive: SQL과 유사한 질의 언어를 제공하여, 하둡 데이터에 쉽게 접근할 수 있게 해줍니다.

    Pig: 데이터 분석을 위한 스크립트 언어입니다. MapReduce 작업을 더 쉽게 구현할 수 있게 해줍니다.

    HBase: 실시간 읽기/쓰기 작업이 가능한 분산 데이터베이스입니다.

    ZooKeeper: 분산 애플리케이션을 위한 중앙 집중식 서비스를 제공하는 코디네이션 툴입니다.

작동 방식:

    데이터 저장: 데이터가 HDFS에 분산 저장됩니다. 각 데이터 블록은 기본적으로 128MB 또는 256MB로 나누어져 여러 데이터노드에 복제됩니다.

    작업 분배: MapReduce 작업을 시작할 때, 'Map' 작업은 여러 노드에 분산되어 실행됩니다.

    결과 집계: 'Reduce' 단계에서 각 노드의 결과를 모아 최종 결과를 생성합니다.

하둡은 빅 데이터 처리에 널리 사용되며, 엔터프라이즈 환경뿐만 아니라 연구, 웹 서비스 등 다양한 분야에서 활용됩니다.