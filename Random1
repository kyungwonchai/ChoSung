import pdfplumber
import re
import numpy as np
import configparser
import os
from typing import List, Dict, Any
from collections import defaultdict

# ==============================================================================
# 데이터 구조 및 헬퍼 클래스
# ==============================================================================
class TextElement:
    def __init__(self, element: Dict[str, Any]):
        self.text = element.get('text', '')
        self.x0, self.top, self.x1, self.bottom = [round(element.get(k, 0), 2) for k in ['x0', 'top', 'x1', 'bottom']]

    def __repr__(self):
        return f"'{self.text}' @ ({self.x0}, {self.top})"

def find_nearest_element(anchor: TextElement, candidates: List[TextElement]) -> TextElement | None:
    if not candidates: return None
    anchor_center = np.array([(anchor.x0 + anchor.x1) / 2, (anchor.top + anchor.bottom) / 2])
    min_dist = float('inf')
    nearest_element = None
    for cand in candidates:
        cand_center = np.array([(cand.x0 + cand.x1) / 2, (cand.top + cand.bottom) / 2])
        dist = np.linalg.norm(anchor_center - cand_center)
        if dist < min_dist:
            min_dist = dist
            nearest_element = cand
    return nearest_element

# ==============================================================================
# ? 새로운 핵심 로직: 분리된 식별자 및 라인 재조립 함수 ?
# ==============================================================================
def merge_and_reconstruct(words: List[TextElement]) -> List[TextElement]:
    """
    분리된 우하단 식별자를 먼저 합치고, 전체 텍스트를 라인 단위로 재조립합니다.
    """
    # --- 1단계: 분리된 우하단 식별자('숫자 X 숫자') 합치기 ---
    merged_words = []
    i = 0
    words.sort(key=lambda w: (w.top, w.x0)) # 정렬 필수
    
    while i < len(words):
        # 'X', 'x', '*'를 찾습니다.
        if words[i].text in ['x', 'X', '*'] and i > 0 and i < len(words) - 1:
            prev_word = words[i-1]
            operator = words[i]
            next_word = words[i+1]
            
            # 좌우 단어가 숫자이고, 가로 20px 이내에 있는지, 세로 위치가 비슷한지 확인
            if (prev_word.text.isdigit() and next_word.text.isdigit() and
                (operator.x0 - prev_word.x1) < 20 and (next_word.x0 - operator.x1) < 20 and
                abs(operator.top - prev_word.top) < 5 and abs(operator.top - next_word.top) < 5):
                
                # 새로운 합쳐진 객체 생성
                combined_text = f"{prev_word.text} {operator.text} {next_word.text}"
                new_word_obj = {
                    'text': combined_text,
                    'x0': prev_word.x0, 'top': operator.top,
                    'x1': next_word.x1, 'bottom': operator.bottom
                }
                # 마지막에 추가했던 좌측 숫자를 제거하고 합쳐진 객체로 대체
                merged_words.pop() 
                merged_words.append(TextElement(new_word_obj))
                i += 2 # 3개 단어를 처리했으므로 인덱스 점프
                continue

        merged_words.append(words[i])
        i += 1
        
    return merged_words


# ==============================================================================
# 메인 분석 함수
# ==============================================================================
def analyze_document_with_learned_spec(pdf_path: str, output_txt_path: str):
    spec_file = 'pageset.ini'
    material_code_pattern = re.compile(r"^\d{4}-\d{6}$")
    top_left_pattern = re.compile(r"^\d{2}[A-Z]?$")
    bottom_right_pattern = re.compile(r"^\d+\s?[xX*]\s?\d+$") # 재조립 후 이 패턴이 매칭됨

    config = configparser.ConfigParser()
    master_spec = None

    if os.path.exists(spec_file):
        try:
            config.read(spec_file)
            master_spec = {k: config.getfloat('MasterSpec', k) for k in config['MasterSpec']}
            print(f"'{spec_file}'에서 학습된 규격을 불러왔습니다.")
        except Exception:
            master_spec = None
            
    if master_spec is None:
        print("1차 탐색: 문서 전체를 스캔하여 '마스터 규격'을 학습합니다...")
        all_distances = []
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages, 1):
                print(f"  - 학습 중... (페이지 {page_num}/{len(pdf.pages)})")
                raw_words = [TextElement(w) for w in page.extract_words()]
                
                # ? 재조립 로직을 학습 단계에서 먼저 적용
                words = merge_and_reconstruct(raw_words)
                
                material_codes = [w for w in words if material_code_pattern.match(w.text)]
                tl_codes = [w for w in words if top_left_pattern.match(w.text)]
                br_codes = [w for w in words if bottom_right_pattern.match(w.text)]

                for mc in material_codes:
                    nearest_tl = find_nearest_element(mc, tl_codes)
                    nearest_br = find_nearest_element(mc, br_codes)
                    
                    if nearest_tl and nearest_br:
                        all_distances.append([
                            nearest_tl.x0 - mc.x0, nearest_tl.top - mc.top,
                            nearest_br.x1 - mc.x1, nearest_br.bottom - mc.bottom
                        ])

        if not all_distances: raise ValueError("문서 전체에서 유효한 규격 샘플을 찾을 수 없습니다.")

        distances_arr = np.array(all_distances)
        q1 = np.percentile(distances_arr, 25, axis=0)
        q3 = np.percentile(distances_arr, 75, axis=0)
        iqr = q3 - q1
        lower_bound, upper_bound = q1 - 1.5 * iqr, q3 + 1.5 * iqr
        filtered_distances = distances_arr[np.all((distances_arr >= lower_bound) & (distances_arr <= upper_bound), axis=1)]
        
        avg_distances = np.mean(filtered_distances, axis=0)
        master_spec = {
            'dist_tl_x': avg_distances[0], 'dist_tl_y': avg_distances[1],
            'dist_br_x': avg_distances[2], 'dist_br_y': avg_distances[3]
        }
        
        print("학습 완료. 마스터 규격을 `pageset.ini` 파일에 저장합니다.")
        config['MasterSpec'] = {k: str(v) for k, v in master_spec.items()}
        with open(spec_file, 'w') as configfile:
            config.write(configfile)

    print("\n2차 탐색: 학습된 규격을 적용하여 전체 데이터를 추출합니다...")
    final_results = []
    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages, 1):
            raw_words = [TextElement(w) for w in page.extract_words()]
            words = merge_and_reconstruct(raw_words) # ? 2차 탐색에서도 동일하게 재조립 적용
            material_codes = [w for w in words if material_code_pattern.match(w.text)]
            
            for mc in material_codes:
                box_x0, box_top, box_x1, box_bottom = (
                    mc.x0 + master_spec['dist_tl_x'], mc.top + master_spec['dist_tl_y'],
                    mc.x1 + master_spec['dist_br_x'], mc.bottom + master_spec['dist_br_y']
                )
                
                words_in_box = [w for w in words if w.x0 >= box_x0 and w.x1 <= box_x1 and w.top >= box_top and w.bottom <= box_bottom]
                
                # ? 최종 정보도 라인 단위로 재구성
                reconstructed_data = " | ".join(sorted([" ".join(w.text for w in line) for y, line in defaultdict(list, [(w.top, line) for w in sorted(words_in_box, key=lambda x: (x.top, x.x0)) for y, line in lines.items() if abs(w.top - y) < 5] or [(w.top, [w]) for _ in [lines := defaultdict(list)]]) for y, line in lines.items()], key=lambda line: float(line.split("' @ (")[1].split(", ")[1].rstrip(')'))))
                
                lines = defaultdict(list)
                for word in sorted(words_in_box, key=lambda w: (w.top, w.x0)):
                    found = False
                    for y_key in lines.keys():
                        if abs(word.top - y_key) < 5:
                            lines[y_key].append(word)
                            found = True
                            break
                    if not found:
                        lines[word.top].append(word)
                
                full_text_lines = []
                for y_key in sorted(lines.keys()):
                    full_text_lines.append(" ".join(w.text for w in lines[y_key]))
                reconstructed_data = " | ".join(full_text_lines)
                
                final_results.append((page_num, mc.text, reconstructed_data))
    
    with open(output_txt_path, 'w', encoding='utf-8') as f:
        f.write(f"--- 학습된 마스터 규격 (from {spec_file}) ---\n")
        for key, value in master_spec.items():
            f.write(f"- {key}: {value:.2f}px\n")
        f.write("-" * 50 + "\n\n")
        
        f.write("--- 최종 추출 데이터 ---\n")
        for page_num, mc_text, data in final_results:
            f.write(f"페이지: {page_num}, 자재코드: {mc_text}, 정보: [ {data} ]\n")

    print(f"\n모든 분석 완료! 최종 결과가 '{output_txt_path}' 파일에 저장되었습니다.")

# --- 메인 실행 부분 ---
if __name__ == "__main__":
    pdf_file_path = "YOUR_PDF_FILE_PATH.pdf"
    output_file_path = "part_list_final_output.txt"
    analyze_document_with_learned_spec(pdf_file_path, output_file_path)