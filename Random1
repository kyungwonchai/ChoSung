# ==============================================================================
# 1단계: 필요한 모든 도구 불러오기
# ==============================================================================
import os
import pandas as pd
from langchain.text_splitter import RecursiveCharacterTextSplitter
from qdrant_client import QdrantClient, models
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ==============================================================================
# 2단계: 설정 및 전역 변수 정의
# ==============================================================================
# ?? 학습시킬 로컬 리액트 프로젝트 폴더 경로를 지정합니다.
REACT_PROJECT_PATH = r"C:\path\to\your\react\project" 

# 이전에 다운로드한 로컬 모델들의 폴더 경로
EMBEDDING_MODEL_PATH = r"C:\my_google_models\embedding_model"
LLM_MODEL_PATH = r"C:\my_google_models\llm_model"

# ==============================================================================
# 3단계: 핵심 기능 함수 정의
# ==============================================================================

def load_files_from_react_project(project_path: str) -> dict:
    """지정된 React 프로젝트 폴더에서 소스 코드 파일들을 읽어 내용을 반환합니다."""
    print(f"'{project_path}' 폴더에서 소스 코드 파일을 읽습니다...")
    
    file_contents = {}
    # 학습할 파일 확장자 목록
    target_extensions = ['.js', '.jsx', '.ts', '.tsx', '.md', '.txt']
    # 무시할 폴더 목록
    ignore_folders = ['node_modules', '.git', 'build', 'dist', 'public']

    # os.walk를 사용하여 모든 하위 폴더와 파일을 순회합니다.
    for root, dirs, files in os.walk(project_path):
        # 무시할 폴더는 건너뛰기
        dirs[:] = [d for d in dirs if d not in ignore_folders]
        
        for file in files:
            # 원하는 확장자를 가진 파일만 처리
            if any(file.endswith(ext) for ext in target_extensions):
                file_path = os.path.join(root, file)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        file_contents[file_path] = f.read()
                except Exception as e:
                    print(f"    - 오류: {file_path} 파일을 읽는 데 실패 - {e}")

    print(f"파일 읽기 완료. 총 {len(file_contents)}개의 소스 코드 파일을 수집했습니다.")
    return file_contents


def process_and_chunk(files: dict) -> list:
    """수집된 파일 내용들을 텍스트를 작은 조각(Chunk)으로 나눕니다."""
    print("데이터 정제 및 청킹(Chunking)을 시작합니다...")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100
    )
    
    all_chunks = []
    for path, content in files.items():
        chunks = text_splitter.split_text(content)
        for chunk in chunks:
            # 각 청크에 출처 파일 경로(source) 메타데이터를 포함하여 저장
            all_chunks.append({"text": chunk, "source": path})
            
    print(f"청킹 완료. 총 {len(all_chunks)}개의 텍스트 청크를 생성했습니다.")
    return all_chunks


def ask_codebase_expert(question: str, qdrant_cli, embed_model, gen_model, gen_tokenizer):
    """사용자 질문에 대해 로컬 코드베이스 내용을 기반으로 답변합니다."""
    print("\n--- 질문에 대한 답변 생성 중 ---")
    
    question_vector = embed_model.encode('query: ' + question).tolist()
    
    hits = qdrant_cli.search(
        collection_name="react_codebase",
        query_vector=question_vector,
        limit=3
    )
    if not hits:
        return "죄송합니다, 해당 질문에 대한 정보를 코드에서 찾을 수 없습니다."
    
    context = ""
    sources = set()
    for hit in hits:
        context += f"// 출처 파일: {hit.payload['source']}\n\n{hit.payload['text']}\n\n"
        sources.add(hit.payload['source'])
        
    messages = [
        {"role": "user", "content": f"당신은 우리 회사 React 프로젝트의 코드 전문가입니다. 아래 '참고 코드'만을 바탕으로 '질문'에 대해 한국어로 상세히 답변해주세요. 답변 마지막에는 출처 파일 경로를 명시해주세요.\n\n### 참고 코드:\n{context}\n\n### 질문:\n{question}"}
    ]
    prompt = gen_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    inputs = gen_tokenizer.encode(prompt, add_special_tokens=True, return_tensors="pt").to(gen_model.device)
    outputs = gen_model.generate(input_ids=inputs, max_new_tokens=512)
    answer = gen_tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    final_answer = answer.split('<start_of_turn>model')[-1].strip()
    final_answer += f"\n\n출처: {', '.join(sources)}"
    
    return final_answer

# ==============================================================================
# 4단계: 메인 실행 블록
# ==============================================================================
if __name__ == "__main__":
    # --- 1. 데이터 수집 및 처리 ---
    source_files = load_files_from_react_project(REACT_PROJECT_PATH)
    text_chunks = process_and_chunk(source_files)

    # --- 2. RAG 시스템 초기화 ---
    print("\n--- RAG 시스템 초기화 시작 ---")
    embedding_model = SentenceTransformer(EMBEDDING_MODEL_PATH, local_files_only=True)
    qdrant_client = QdrantClient(":memory:")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    llm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_PATH, local_files_only=True)
    llm_model = AutoModelForCausalLM.from_pretrained(LLM_MODEL_PATH, local_files_only=True).to(device)
    print("--- 모든 모델 로드 완료 ---")

    # --- 3. Qdrant에 데이터 주입 ---
    qdrant_client.recreate_collection(
        collection_name="react_codebase",
        vectors_config=models.VectorParams(size=embedding_model.get_sentence_embedding_dimension(), distance=models.Distance.COSINE)
    )
    
    points_to_ingest = []
    for i, chunk_data in enumerate(text_chunks):
        vector = embedding_model.encode('passage: ' + chunk_data['text']).tolist()
        points_to_ingest.append(models.PointStruct(id=i, vector=vector, payload=chunk_data))

    qdrant_client.upsert(collection_name="react_codebase", points=points_to_ingest, wait=True)
    print(f"--- {len(points_to_ingest)}개의 청크를 Qdrant에 주입 완료. 시스템 준비 완료 ---")

    # --- 4. 시스템 테스트 ---
    test_question = "우리 프로젝트에서 사용하는 주된 상태 관리 라이브러리는 뭐야?"
    final_response = ask_codebase_expert(test_question, qdrant_client, embedding_model, llm_model, llm_tokenizer)
    
    print("\n" + "="*50)
    print(f"질문: {test_question}")
    print(f"답변:\n{final_response}")
    print("="*50)