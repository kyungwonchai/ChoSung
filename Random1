ì—ì„œ ê°€ì¥ ì •í™•í•˜ê³  ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë‚´ê¸° ìœ„í•´ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.
ì—¬ê¸°ì„œëŠ” PyTorch ê¸°ë°˜ Transformer ëª¨ë¸ì„ í™œìš©í•˜ì—¬ QR ì½”ë“œì—ì„œ ëª¨ë¸ëª…ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“­ë‹ˆë‹¤.

ğŸ”¥ ì‹¤ì „ ì ‘ê·¼ ë°©ì‹ (ìºê¸€ ìˆ˜ì¤€)
QR ì½”ë“œì™€ ëª¨ë¸ëª… ë°ì´í„° ì „ì²˜ë¦¬

QR ì½”ë“œë¥¼ í† í°í™”(Tokenization) ë° ë²¡í„°í™”(Embedding)
ëª¨ë¸ëª…ì„ ë ˆì´ë¸” ì¸ì½”ë”©(Label Encoding)
Transformer ê¸°ë°˜ ëª¨ë¸ ìƒì„±

QR ì½”ë“œ ì…ë ¥ì„ Transformer ê¸°ë°˜ ì‹œí€€ìŠ¤ ì¸ì½”ë”(Embedding + Self-Attention) ì²˜ë¦¬
ìµœì¢…ì ìœ¼ë¡œ Softmax ê¸°ë°˜ ëª¨ë¸ ë¶„ë¥˜
ëª¨ë¸ í•™ìŠµ

CrossEntropyLoss + AdamW ìµœì í™”
GPUë¥¼ í™œìš©í•˜ì—¬ ë¹ ë¥´ê²Œ í•™ìŠµ ì§„í–‰
ëª¨ë¸ í‰ê°€ ë° í…ŒìŠ¤íŠ¸

ìƒˆë¡œìš´ QR ì…ë ¥ ì‹œ ëª¨ë¸ëª…ì„ ì˜ˆì¸¡í•˜ì—¬ ì¶œë ¥
âœ… ì‹¤ì „ìš© ì½”ë“œ (PyTorch, Transformers, ìºê¸€ ìŠ¤íƒ€ì¼)
ì‚¬ìš© í™˜ê²½: Python 3.9.13 / PyTorch / GPU ì‚¬ìš© ê°€ëŠ¥
ìµœì í™” ê¸°ë²•: AdamW / Learning Rate Scheduler
ëª¨ë¸ êµ¬ì¡°: Transformer ê¸°ë°˜ Sequence Classifier

python
ì½”ë“œ ë³µì‚¬
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizerFast

# GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# âœ… 1. í† í¬ë‚˜ì´ì € ì¤€ë¹„ (BERT ê¸°ë°˜ í† í°í™” ì‚¬ìš©)
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

# âœ… 2. ë°ì´í„°ì…‹ ì •ì˜
class QRDataset(Dataset):
    def __init__(self, qr_codes, labels, tokenizer, max_len=32):
        self.qr_codes = qr_codes
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.qr_codes)

    def __getitem__(self, idx):
        qr_text = str(self.qr_codes[idx])
        label = self.labels[idx]

        # QR í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  ID ë³€í™˜
        encoding = self.tokenizer(
            qr_text, 
            padding="max_length", 
            truncation=True, 
            max_length=self.max_len, 
            return_tensors="pt"
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "label": torch.tensor(label, dtype=torch.long)
        }

# âœ… 3. Transformer ê¸°ë°˜ ëª¨ë¸ ì •ì˜
class QRModel(nn.Module):
    def __init__(self, num_labels):
        super(QRModel, self).__init__()
        self.embedding = nn.Embedding(30522, 128)  # BERT í† í° ê°œìˆ˜ë§Œí¼ ì„¤ì •
        self.encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8)
        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=4)
        self.fc = nn.Linear(128, num_labels)  # ìµœì¢… ë¶„ë¥˜ê¸°

    def forward(self, input_ids, attention_mask):
        x = self.embedding(input_ids)  # ì„ë² ë”© ë ˆì´ì–´
        x = self.transformer(x)  # Transformer Encoder
        x = x.mean(dim=1)  # í‰ê·  í’€ë§
        output = self.fc(x)  # ë¶„ë¥˜ ë ˆì´ì–´
        return output

# âœ… 4. í•™ìŠµ í•¨ìˆ˜ ì •ì˜
def train_model(model, train_loader, criterion, optimizer, num_epochs=5):
    model.train()
    for epoch in range(num_epochs):
        total_loss = 0
        for batch in train_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["label"].to(device)

            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}")

# âœ… 5. ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜
def predict(model, qr_text):
    model.eval()
    with torch.no_grad():
        encoding = tokenizer(
            qr_text, 
            padding="max_length", 
            truncation=True, 
            max_length=32, 
            return_tensors="pt"
        )
        input_ids = encoding["input_ids"].to(device)
        attention_mask = encoding["attention_mask"].to(device)

        output = model(input_ids, attention_mask)
        predicted_label = torch.argmax(output, dim=1).item()

    return predicted_label

# âœ… 6. ë°ì´í„° ì¤€ë¹„
qr_samples = ["QR12345ABC", "QR67890XYZ", "QRAABBCC123"]
labels = [0, 1, 2]  # ëª¨ë¸ëª…ì„ ìˆ«ìë¡œ ë§¤í•‘

# ë°ì´í„° ë¡œë” ìƒì„±
dataset = QRDataset(qr_samples, labels, tokenizer)
train_loader = DataLoader(dataset, batch_size=2, shuffle=True)

# âœ… 7. ëª¨ë¸ í•™ìŠµ ì‹¤í–‰
model = QRModel(num_labels=3).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=3e-4)

train_model(model, train_loader, criterion, optimizer, num_epochs=10)

# âœ… 8. ì˜ˆì¸¡ ì‹¤í–‰
new_qr = "QR67890XYZ"
predicted_model = predict(model, new_qr)
print(f"ğŸ” ì˜ˆì¸¡ëœ ëª¨ë¸: {predicted_model}")
ğŸ”¥ âœ… ì‹¤ì „ ì½”ë“œ íŠ¹ì§•
Transformer ê¸°ë°˜ ëª¨ë¸ ì‚¬ìš©

PyTorch nn.TransformerEncoderë¥¼ í™œìš©í•˜ì—¬ QR ë¬¸ìì—´ì„ ë²¡í„°í™” í›„ í•™ìŠµ
ê¸°ì¡´ CNN/LSTMë³´ë‹¤ í›¨ì”¬ ê°•ë ¥í•œ ì„±ëŠ¥
BERT Tokenizer ì‚¬ìš©

transformers.BertTokenizerFastë¡œ QR ì½”ë“œë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í† í°í™”
QRì— í¬í•¨ëœ íŠ¹ìˆ˜ë¬¸ì, ìˆ«ì ë“±ì„ ê·¸ëŒ€ë¡œ ë°˜ì˜ ê°€ëŠ¥
CrossEntropy + AdamW ìµœì í™” ì ìš©

ì¼ë°˜ Adamë³´ë‹¤ ì„±ëŠ¥ ì¢‹ì€ AdamW ì‚¬ìš©
GPU ì§€ì› (ìë™ ê°ì§€)

torch.device("cuda" if torch.cuda.is_available() else "cpu")
GPU ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ì‚¬ìš©
ğŸ”¥ âœ… ì‹¤ì „ ëª¨ë¸ ì„±ëŠ¥ ìµœì í™” (ì¶”ê°€)
ğŸ”¹ ë°ì´í„°ë¥¼ 100ë§Œ ê°œ ì´ìƒìœ¼ë¡œ í™•ì¥
ğŸ”¹ í•™ìŠµ ë°ì´í„°ì…‹ì„ CSVì—ì„œ ìë™ ë¡œë“œí•˜ë„ë¡ ë³€ê²½
ğŸ”¹ LSTM, CNN ë“± ì¶”ê°€ ë¹„êµ ì‹¤í—˜ ì§„í–‰
ğŸ”¹ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (learning rate, batch size ë“± ì¡°ì •)
ğŸ”¹ Pretrained BERT ì‚¬ìš© ê°€ëŠ¥ (Huggingface bert-base-uncased)
ğŸ”¥ âœ… ê²°ë¡ 
ì´ ëª¨ë¸ì€ ìºê¸€ ìˆ˜ì¤€ìœ¼ë¡œ í™œìš© ê°€ëŠ¥í•œ ì‹¤ì „ ì½”ë“œì…ë‹ˆë‹¤.
Transformer ê¸°ë°˜ ëª¨ë¸ì´ë¯€ë¡œ ì‹¤ì œ ì‚°ì—… í˜„ì¥ì—ì„œ QR â†’ ëª¨ë¸ëª… ë¶„ë¥˜ ì„±ëŠ¥ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤.
ë°ì´í„°ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ì„±ëŠ¥ì´ ë”ìš± í–¥ìƒë©ë‹ˆë‹¤.