import pdfplumber
import re
import statistics
from typing import List, Dict, Any

# ==============================================================================
# 데이터 구조 클래스 (이전과 동일)
# ==============================================================================
class TextElement:
    def __init__(self, element: Dict[str, Any]):
        self.value = element.get('text', '')
        self.x0 = round(element.get('x0', 0), 2)
        self.y0 = round(element.get('y0', 0), 2)
        self.x1 = round(element.get('x1', 0), 2)
        self.y1 = round(element.get('y1', 0), 2)
        self.size = round(element.get('size', 0), 2)
        self.color = element.get('non_stroking_color', (0, 0, 0))

    def __repr__(self) -> str:
        # 출력 가독성을 위해 대표 정보만 간략히 표시
        return f"'{self.value}' (y0:{self.y0}, y1:{self.y1})"

class PartComponent:
    def __init__(self, page_number: int, part_number_element: TextElement):
        self.page_number = page_number
        self.part_number = part_number_element
        self.related_elements: List[TextElement] = []

    def add_element(self, element: TextElement):
        self.related_elements.append(element)

    def __repr__(self) -> str:
        # 최종 출력 시, 시각적 순서로 정렬
        sorted_elements = sorted(self.related_elements, key=lambda e: (e.y0, e.x0))
        details_str = ",\n\t".join([
            f"Color: {e.color}, Size: {e.size}, Value: '{e.value}', Position: (x0={e.x0}, y0={e.y0}, x1={e.x1}, y1={e.y1})" 
            for e in sorted_elements
        ])
        return (
            f"Page: {self.page_number}, PartNumber: {self.part_number.value}\n"
            f"Details: [\n\t{details_str}\n]"
        )

# ==============================================================================
# ? 완전히 새로 작성된 메인 분석 함수 ?
# ==============================================================================
def analyze_parts_from_pdf(pdf_path: str, output_txt_path: str):
    """
    지능형 클러스터링 알고리즘으로 PDF에서 부품 정보를 정확히 그룹화합니다.
    """
    all_part_components = []
    part_number_pattern = re.compile(r"^\d{4}-\d{6}$")

    print(f"PDF 분석 시작: {pdf_path}")
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages, 1):
                print(f"\n--- 페이지 {page_num} 처리 시작 ---")

                # 1. 페이지를 100px 위에서부터 자릅니다.
                page_within_bounds = page.crop((0, 100, page.width, page.height))
                words = page_within_bounds.extract_words(extra_attrs=["size", "non_stroking_color"])
                
                if not words:
                    print(f"페이지 {page_num}에서 처리할 단어를 찾지 못했습니다.")
                    continue
                
                print(f"페이지 {page_num}에서 {len(words)}개의 단어 추출. (첫 단어 y0: {words[0]['y0']:.2f}, > 100 확인)")

                # 2. 수직 위치(y0) 기준으로 모든 단어를 정렬합니다.
                sorted_words = sorted(words, key=lambda w: w['y0'])

                # 3. 페이지의 표준 줄 간격을 '학습'합니다.
                line_gaps = []
                for i in range(len(sorted_words) - 1):
                    # 같은 줄에 있는 단어들을 건너뛰기 위해 y좌표가 비슷한 경우 제외
                    if abs(sorted_words[i]['y0'] - sorted_words[i+1]['y0']) > 5:
                        gap = sorted_words[i+1]['y0'] - sorted_words[i]['y1']
                        if gap > 0: # 겹치지 않는 경우만
                            line_gaps.append(gap)
                
                if not line_gaps:
                    # 페이지에 텍스트가 한 줄 뿐인 극단적인 경우
                    median_gap = 10 
                else:
                    median_gap = statistics.median(line_gaps)
                
                # 그룹을 나누는 기준이 될 '큰 공백' 임계값 설정 (표준 간격의 1.8배)
                separation_threshold = median_gap * 1.8 + 2 # 최소 여유 2px 추가
                print(f"페이지 {page_num}의 표준 줄 간격: {median_gap:.2f}px, 그룹 분리 임계값: {separation_threshold:.2f}px")

                # 4. 임계값을 기준으로 단어들을 여러 '블록'으로 나눕니다.
                blocks = []
                if sorted_words:
                    current_block = [sorted_words[0]]
                    for i in range(len(sorted_words) - 1):
                        current_word = sorted_words[i]
                        next_word = sorted_words[i+1]
                        
                        # ? 핵심 로직: 단어의 '하단'과 다음 단어의 '상단' 사이의 실제 공백 계산
                        vertical_gap = next_word['y0'] - current_word['y1']
                        
                        if vertical_gap > separation_threshold:
                            blocks.append(current_block)
                            current_block = [next_word]
                        else:
                            current_block.append(next_word)
                    blocks.append(current_block)

                print(f"페이지 {page_num}에서 {len(blocks)}개의 정보 블록을 감지했습니다.")

                # 5. 분리된 각 블록을 독립적으로 처리하여 최종 결과 생성
                for block in blocks:
                    found_part_number_obj = None
                    for word_obj in block:
                        if part_number_pattern.match(word_obj.get('text', '')):
                            found_part_number_obj = word_obj
                            break
                    
                    if found_part_number_obj:
                        part_element = TextElement(found_part_number_obj)
                        component = PartComponent(page_num, part_element)
                        for word_obj in block:
                            component.add_element(TextElement(word_obj))
                        all_part_components.append(component)

        # 6. 최종 결과를 파일에 저장
        with open(output_txt_path, 'w', encoding='utf-8') as f:
            f.write(f"총 {len(all_part_components)}개의 부품을 찾았습니다.\n" + "="*60 + "\n")
            for comp in all_part_components:
                f.write(str(comp) + "\n" + "-"*60 + "\n")
        print(f"\n분석 완료! 결과가 '{output_txt_path}' 파일에 저장되었습니다.")

    except Exception as e:
        print(f"스크립트 실행 중 오류가 발생했습니다: {e}")

# --- 메인 실행 부분 ---
if __name__ == "__main__":
    pdf_file_path = "YOUR_PDF_FILE_PATH.pdf" 
    output_file_path = "part_list_final_output.txt"
    analyze_parts_from_pdf(pdf_file_path, output_file_path)