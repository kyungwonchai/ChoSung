import pdfplumber
import re
from typing import List, Dict, Any
from collections import defaultdict
import numpy as np
from sklearn.cluster import KMeans

# ==============================================================================
# 데이터 구조 클래스 (변경 없음)
# ==============================================================================
class TextElement:
    def __init__(self, element: Dict[str, Any]):
        self.value = element.get('text', '')
        self.x0, self.top, self.x1, self.bottom = [round(element.get(k, 0), 2) for k in ['x0', 'top', 'x1', 'bottom']]
        self.size = round(element.get('size', 0), 2)
        self.color = element.get('non_stroking_color', (0, 0, 0))

class PartComponent:
    def __init__(self, page_number: int, part_number_element: TextElement):
        self.page_number = page_number
        self.part_number = part_number_element
        self.related_elements: List[TextElement] = []

    def add_element(self, element: TextElement):
        self.related_elements.append(element)

    def get_details_string(self) -> str:
        if not self.related_elements: return "No other details found."
        sorted_elements = sorted(self.related_elements, key=lambda e: (e.top, e.x0))
        details_list = [
            f"Color: {e.color}, Size: {e.size}, Value: '{e.value}', Position: (x0={e.x0}, top={e.top}, x1={e.x1}, bottom={e.bottom})"
            for e in sorted_elements
        ]
        return ",\n\t".join(details_list)

# ==============================================================================
# ? 2단계 학습 및 적용 로직이 포함된 메인 함수 ?
# ==============================================================================
def analyze_parts_from_pdf(pdf_path: str, output_txt_path: str):
    all_part_components = []
    part_number_pattern = re.compile(r"^\d{4}-\d{6}$")

    print(f"PDF 분석 시작: {pdf_path}")
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages, 1):
                print(f"\n--- 페이지 {page_num} 처리 시작 ---")
                
                raw_words = page.extract_words(extra_attrs=["size", "non_stroking_color"])
                sanitized_words = [w for w in raw_words if 'top' in w and 'bottom' in w and w['bottom'] > w['top']]

                if len(sanitized_words) < 2:
                    print(f"페이지 {page_num}에서 분석할 텍스트가 부족합니다.")
                    continue
                
                sorted_words = sorted(sanitized_words, key=lambda w: w['top'])

                # --- 1단계: 그룹 '기준' 학습 (Pass 1) ---
                print("1단계: 페이지 레이아웃 학습 시작...")
                gaps = np.array([
                    sorted_words[i+1]['top'] - sorted_words[i]['bottom']
                    for i in range(len(sorted_words) - 1)
                    if sorted_words[i+1]['top'] > sorted_words[i]['bottom']
                ]).reshape(-1, 1)

                if len(gaps) < 2:
                    print("페이지의 줄 간격이 하나뿐이라 그룹핑을 건너뜁니다.")
                    continue

                # K-Means 클러스터링으로 간격들을 '작은 간격 그룹'과 '큰 간격 그룹'으로 분류
                kmeans = KMeans(n_clusters=2, random_state=0, n_init='auto').fit(gaps)
                
                # 각 클러스터의 중심(평균 간격) 확인
                center1, center2 = kmeans.cluster_centers_.flatten()
                small_gap_cluster_center = min(center1, center2)
                large_gap_cluster_center = max(center1, center2)
                
                # '큰 간격'으로 분류된 값들 중 가장 작은 값을 그룹 분리 기준으로 설정
                separation_threshold = gaps[kmeans.labels_ == np.argmax([center1, center2])].min()
                print(f"학습 완료: 내부 줄 간격(평균 {small_gap_cluster_center:.2f}px), 그룹 분리 간격(최소 {separation_threshold:.2f}px)")

                # --- 2단계: 학습된 '기준' 적용 (Pass 2) ---
                print("2단계: 학습된 기준으로 그룹핑 적용...")
                blocks, current_block = [], [sorted_words[0]]
                for i in range(len(sorted_words) - 1):
                    gap = sorted_words[i+1]['top'] - sorted_words[i]['bottom']
                    # 학습된 기준보다 간격이 크거나 같으면 그룹 분리
                    if gap >= separation_threshold:
                        blocks.append(current_block)
                        current_block = [sorted_words[i+1]]
                    else:
                        current_block.append(sorted_words[i+1])
                blocks.append(current_block)
                
                print(f"페이지 {page_num}에서 {len(blocks)}개의 정보 블록을 감지했습니다.")

                # 블록 처리 및 결과 생성
                for block in blocks:
                    found_part_number_obj = next((w for w in block if part_number_pattern.match(w.get('text', ''))), None)
                    if found_part_number_obj:
                        part_element = TextElement(found_part_number_obj)
                        component = PartComponent(page_num, part_element)
                        for word_obj in block:
                            if word_obj is not found_part_number_obj: component.add_element(TextElement(word_obj))
                        all_part_components.append(component)

        # 최종 결과 출력
        page_map = defaultdict(list)
        for component in all_part_components: page_map[component.page_number].append(component)

        with open(output_txt_path, 'w', encoding='utf-8') as f:
            f.write(f"총 {len(all_part_components)}개의 부품을 {len(page_map)}개 페이지에서 찾았습니다.\n")
            for page_num in sorted(page_map.keys()):
                components_on_page = page_map[page_num]
                f.write("\n" + "="*25 + f" PAGE {page_num} " + "="*25 + "\n")
                f.write(f"({len(components_on_page)}개의 부품 발견)\n\n")
                for i, component in enumerate(components_on_page, 1):
                    f.write(f"--- {i}. PartNumber: {component.part_number.value} ---\n")
                    f.write(f"Details: [\n\t{component.get_details_string()}\n]\n")
                    f.write("-" * 60 + "\n")

        print(f"\n분석 완료! 결과가 '{output_txt_path}' 파일에 저장되었습니다.")

    except Exception as e:
        print(f"스크립트 실행 중 오류가 발생했습니다: {e}")
        import traceback; traceback.print_exc()

# --- 메인 실행 부분 ---
if __name__ == "__main__":
    pdf_file_path = "YOUR_PDF_FILE_PATH.pdf" 
    output_file_path = "part_list_final_output.txt"
    analyze_parts_from_pdf(pdf_file_path, output_file_path)