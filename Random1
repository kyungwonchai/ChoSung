# Primary Key로 사용할 컬럼 이름 (코드가 강제로 'Part Name'으로 지정하므로 바꿀 필요 없음)
PRIMARY_KEY_COLUMN = 'Part Name'              

# --------------------------------------------------------------------

def sanitize_col_name(name):
    """DB 컬럼명으로 사용 가능하게 이름 변경 (소문자, 공백->_)"""
    return name.strip().lower().replace(' ', '_').replace('/', '_')

def main():
    """메인 실행 함수"""
    print("--- 작업을 시작합니다. ---")
    
    # 1. CSV 파일 읽기
    try:
        with open(CSV_FILE_PATH, mode='r', encoding='utf-8') as f:
            reader = csv.reader(f)
            
            # CSV의 가장 첫 줄을 그대로 읽습니다.
            raw_header = next(reader)
            
            # ??? 이 부분이 핵심: 첫 번째 헤더(A1셀) 값을 'Part Name'으로 강제 치환합니다. ???
            if raw_header:
                raw_header[0] = 'Part Name'
            
            # 강제 치환된 헤더를 DB 컬럼명에 맞게 변환합니다.
            header = [sanitize_col_name(h) for h in raw_header]
            
            # 나머지 모든 줄을 데이터로 읽어들입니다.
            all_data_rows = [row for row in reader if row] # 비어있는 행은 제외
        
        if not all_data_rows:
            print(f"?? 오류: CSV 파일 '{CSV_FILE_PATH}'에 데이터가 없습니다.")
            return

        print(f"? CSV 파일 읽기 성공: {len(all_data_rows)}개의 데이터 행을 찾았습니다.")
        primary_key_sql = sanitize_col_name(PRIMARY_KEY_COLUMN)

        if primary_key_sql not in header:
            # 이 오류는 이제 발생하지 않아야 합니다.
            print(f"?? 오류: Primary Key로 지정된 '{PRIMARY_KEY_COLUMN}' 컬럼을 CSV 헤더에서 찾을 수 없습니다.")
            return
            
    except FileNotFoundError:
        print(f"?? 오류: CSV 파일 '{CSV_FILE_PATH}'을(를) 찾을 수 없습니다.")
        return
    except Exception as e:
        print(f"?? 오류: CSV 파일을 읽는 중 예외가 발생했습니다 - {e}")
        return

    conn = None
    try:
        # 2. 데이터베이스 연결
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()
        print(f"? 데이터베이스 '{DB_CONFIG['dbname']}'에 성공적으로 연결했습니다.")

        # 3. 기존 테이블 삭제
        print(f"   - 테이블 '{TABLE_NAME}'이(가) 존재하면 삭제합니다...")
        cursor.execute(f'DROP TABLE IF EXISTS "{TABLE_NAME}";')

        # 4. 새 테이블 생성 (헤더 기반)
        column_definitions = [f'"{col}" TEXT' for col in header]
        pk_index = header.index(primary_key_sql)
        column_definitions[pk_index] += ' PRIMARY KEY'

        create_table_sql = f'CREATE TABLE "{TABLE_NAME}" ({", ".join(column_definitions)});'
        
        print(f"   - 테이블 '{TABLE_NAME}'을(를) 새로 생성합니다...")
        cursor.execute(create_table_sql)

        # 5. 모든 데이터를 한번에 삽입 (Batch Insert)
        placeholders = ', '.join(['%s'] * len(header))
        columns_str = ", ".join(f'"{h}"' for h in header)
        insert_sql = f'INSERT INTO "{TABLE_NAME}" ({columns_str}) VALUES ({placeholders})'
        
        print(f"   - {len(all_data_rows)}개의 데이터를 테이블에 삽입합니다...")
        psycopg2.extras.execute_batch(cursor, insert_sql, all_data_rows)

        # 6. 모든 변경사항 최종 저장
        conn.commit()
        print("\n?? 모든 작업이 성공적으로 완료되었습니다!")

    except psycopg2.Error as e:
        print(f"\n?????? 데이터베이스 작업 중 오류가 발생했습니다! ??????")
        print(f"오류 코드: {e.pgcode}")
        print(f"오류 내용: {e.pgerror}")
        if conn:
            conn.rollback()

    finally:
        if conn:
            conn.close()
        print("--- 작업을 종료합니다. ---")

if __name__ == "__main__":
    main()
## 변경된 점 설명
raw_header = next(reader): 일단 CSV 파일의 첫 줄을 그대로 읽습니다.

raw_header[0] = 'Part N