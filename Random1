import React from 'react';
import CodeBlock from '../CodeBlock';

const HighlightedText: React.FC<{children: React.ReactNode}> = ({ children }) => (
    <span className="text-yellow-300 font-semibold">{children}</span>
);

const pythonCode = `import os
import json
import numpy as np
import in_house_ai_sdk as ai_sdk # 가상의 사내 AI SDK 임포트

# --- 1. 초기 설정 및 가상 DB 로드 ---
# API 키 설정 및 Vector DB 로드는 백엔드 서버 시작 시 완료되었다고 가정합니다.
# ai_sdk.configure(api_key=os.environ.get("IN_HOUSE_API_KEY"))
# vector_db = load_vector_db()

# 이 예제에서는 개념 이해를 위해 DB를 직접 정의합니다.
vector_db = {
    "CHUNK-001": {
        "text": "SM-482 장비의 노즐 막힘은 주로 이물질이나 솔더볼에 의해 발생합니다. 노즐 클리닝 핀을 사용하여 이물질을 제거하고, 막힘이 심한 경우 노즐을 교체해야 합니다.",
        "vector": [0.1, 0.2, -0.1, 0.9, -0.4], # 실제로는 768차원 등의 고차원 벡터
        "metadata": { "source": "SM-482_manual.pdf", "page": 56, "equipment": "SM-482", "image_url": "..." }
    },
    "CHUNK-002": {
        "text": "진공 펌프 압력 저하 시 흡착 실패가 발생할 수 있습니다. 펌프의 압력 게이지를 확인하고 필터를 교체하십시오.",
        "vector": [-0.5, 0.8, 0.2, -0.1, 0.1], # 실제로는 768차원 등의 고차원 벡터
        "metadata": { "source": "troubleshooting_guide.docx", "page": 12, "equipment": "all", "image_url": None }
    }
}

# --- 2. RAG 파이프라인의 주요 함수들 ---

def retrieve_context(user_question: str, db: dict) -> list:
    """사용자 질문과 가장 유사한 문서를 DB에서 검색 (Retrieval)"""
    print("1. 사용자 질문을 벡터로 변환...")
    result = ai_sdk.embed_content(model="in-house-embedding-v2", content=user_question)
    query_vector = np.array(result['embedding'])
    
    # DB의 벡터들과 코사인 유사도 계산
    similarities = { id: np.dot(query_vector, np.array(data['vector'])) for id, data in db.items() }
    
    # 유사도 높은 순으로 정렬하여 상위 문서 반환
    sorted_ids = sorted(similarities, key=similarities.get, reverse=True)
    retrieved_docs = [db[id] for id in sorted_ids[:2]] # 상위 2개 문서 선택
    print("2. 관련 문서 검색 완료.")
    return retrieved_docs

def generate_answer_with_llm(user_question: str, context_docs: list) -> str:
    """검색된 컨텍스트를 바탕으로 LLM을 이용해 최종 답변 생성 (Generation)"""
    context_str = "\\n\\n".join([
        f"Source: {doc['metadata']['source']}\\nContent: {doc['text']}" for doc in context_docs
    ])

    prompt = f"""당신은 SMD 공정 전문가 AI입니다.
주어진 '참고 자료'에만 근거하여 사용자의 '질문'에 대한 해결책을 JSON 형식으로 알려주세요.

--- 참고 자료 ---
{context_str}

--- 질문 ---
{user_question}

--- 답변 (JSON 형식) ---
"""
    
    print("3. LLM에 구조화된 답변 요청 중...")
    response = ai_sdk.generate_content(
        model="in-house-llm-v3-instruct",
        prompt=prompt,
        generation_config={"response_mime_type": "application/json"} # JSON 출력 요청
    )
    
    print("4. 최종 답변 생성 완료!")
    return response['text'] # LLM이 생성한 JSON 문자열을 반환

# --- 3. 파이프라인 실행 예시 ---
question = "SM-482 장비에서 부품 흡착 불량이 계속 발생해요. 어떻게 해야 하나요?"

# 1. 검색(Retrieval)
context = retrieve_context(question, vector_db)

# 2. 생성(Generation)
final_json_response_str = generate_answer_with_llm(question, context)

print("\\n--- AI 에이전트 최종 출력 (JSON 형식의 문자열) ---")
print(final_json_response_str)

# 참고: 실제 서버 환경에서는 이 모든 과정이 웹 요청에 대한 응답으로 비동기(async)로 처리되어야 합니다.
`;

const Step6_AgentImplementation: React.FC = () => {
  return (
    <div className="space-y-4">
      <p>
        이제 설계된 아키텍처를 실제 코드로 구현합니다. 이 단계에서는 백엔드(Python)에서 <HighlightedText>사용자 질문을 벡터로 변환하고, Vector DB에서 관련 문서를 검색한 뒤, 검색된 내용을 바탕으로 LLM에게 최종 답변을 생성</HighlightedText>하도록 요청하는 RAG 파이프라인을 완성합니다.
      </p>
      
      <h3 className="text-xl font-semibold text-cyan-400 pt-4">구현 개념 설명 (Python)</h3>
      <p>
        아래 코드는 RAG 파이프라인의 두 핵심 축인 <HighlightedText>검색(Retrieval)</HighlightedText>과 <HighlightedText>생성(Generation)</HighlightedText>의 개념을 실제 코드 형식으로 보여줍니다. Step 4에서 만든 가상의 Vector DB를 어떻게 활용하는지, 그리고 LLM에 어떻게 질문하여 구조화된 답변을 얻어내는지에 집중합니다. 특정 API에 종속되지 않은 <HighlightedText>가상의 사내 AI SDK (`in-house-ai-sdk`)</HighlightedText>를 사용하여 로직의 흐름을 이해하는 데 초점을 맞춥니다.
      </p>
      <CodeBlock code={pythonCode} language="python" />
      <h3 className="text-xl font-semibold text-cyan-400 pt-4">핵심 구현 포인트</h3>
      <ul className="list-disc list-inside space-y-2 pl-4">
        <li>
          <span className="font-bold text-green-400">가상 SDK 사용:</span> 특정 회사의 SDK가 아닌, 개념 설명을 위한 `in-house-ai-sdk`를 사용합니다. 이를 통해 API가 바뀌더라도 RAG의 핵심 로직은 동일하다는 것을 이해할 수 있습니다.
        </li>
        <li>
          <span className="font-bold text-green-400">고도화된 프롬프트 엔지니어링:</span> LLM에게 <HighlightedText>명확한 역할('SMD 공정 전문가'), 지침('참고 자료에만 근거'), 그리고 최종 출력물인 JSON 형식</HighlightedText>을 프롬프트 내에 명시적으로 제공합니다. 이것이 원하는 결과를 얻는 가장 중요한 기술입니다.
        </li>
        <li>
          <span className="font-bold text-green-400">구조화된 데이터 출력:</span> `generation_config`에서 <HighlightedText>`"response_mime_type": "application/json"`</HighlightedText> 옵션을 사용해 LLM이 프롬프트에 명시된 JSON 형식의 답변을 생성하도록 강력하게 유도합니다. 이는 프론트엔드(UI)에서 다루기 쉬운 안정적인 데이터를 얻는 핵심입니다.
        </li>
        <li>
          <span className="font-bold text-green-400">비동기 처리 개념:</span> 코드 예제는 설명을 위해 동기적으로 작성되었지만, 실제 웹 서버 환경에서는 여러 요청을 동시에 효율적으로 처리하기 위해 임베딩 및 LLM 호출과 같은 네트워크 작업은 반드시 <HighlightedText>비동기(asynchronous) 방식</HighlightedText>(예: Python의 `async/await`)으로 구현해야 합니다.
        </li>
      </ul>
    </div>
  );
};