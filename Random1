네, 구글을 선호하신다면 Gemma 모델을 사용하는 것이 가장 좋습니다. Gemma는 구글 제미나이(Gemini)의 기술을 기반으로 만들어진 고성능 오픈소스 모델입니다.

사용자의 6GB VRAM (RTX 3050) 환경에 맞춰, '번역가(임베딩)'와 '리포트 작성가(LLM)' 역할을 할 구글 기반의 추천 모델과 전체 코드를 알려드리겠습니다.

## 1. 사용할 구글 기반 모델 ??
번역가 (Embedding Model): intfloat/multilingual-e5-large

구글의 BERT 기술을 기반으로 만들어진 최고 수준의 다국어 임베딩 모델입니다. 한국어 질문의 의미를 매우 정확하게 벡터로 '번역'하여 Qdrant의 검색 정확도를 높여줍니다.

리포트 작성가 (LLM Generator): google/gemma-2b-it

구글이 직접 만든 Gemma 모델입니다. 2b는 20억 파라미터 모델을 의미하며, 6GB VRAM에 무리 없이 들어가는 크기입니다. -it는 Instruction Tuned의 약자로, RAG처럼 지시를 내리는 작업에 최적화되어 있습니다.

## 2. 모델 다운로드 방법
이전과 마찬가지로, 방화벽 환경을 고려해 아래 두 모델을 인터넷이 되는 곳에서 직접 다운로드합니다.

임베딩 모델 다운로드 (intfloat/multilingual-e5-large)

저장할 폴더 생성: C:/my_google_models/embedding_model

터미널에서 실행:

Bash

git lfs install
cd C:/my_google_models/embedding_model
git clone https://huggingface.co/intfloat/multilingual-e5-large .
LLM 모델 다운로드 (google/gemma-2b-it)

(중요) Gemma는 Llama 3처럼 사용자 동의가 필요한 모델입니다.

먼저 Gemma 2B IT 모델 페이지에 방문하여 라이선스에 동의해야 합니다.

터미널에서 huggingface-cli login으로 미리 로그인해두어야 합니다.

저장할 폴더 생성: C:/my_google_models/llm_model

터미널에서 실행:

Bash

cd C:/my_google_models/llm_model
git clone https://huggingface.co/google/gemma-2b-it .
## 3. 구글 Gemma 모델을 적용한 전체 Python 코드
모델 로드 부분을 구글 모델 경로로 변경하고, 각 모델의 특성에 맞게 코드를 약간 수정했습니다.

Python

import pandas as pd
import psycopg2
from psycopg2.extras import RealDictCursor
from qdrant_client import QdrantClient, models
from sentence_transformers import SentenceTransformer
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch
import re

# --- 1. DB에서 데이터 로드 (동일) ---
def load_data_from_db(conn_info: dict) -> pd.DataFrame:
    # ... (이전 코드와 동일)
    try:
        print("데이터베이스에 연결합니다...")
        conn = psycopg2.connect(**conn_info)
        cursor = conn.cursor(cursor_factory=RealDictCursor)
        query = "SELECT * FROM parts_data;"
        cursor.execute(query)
        rows = cursor.fetchall()
        cursor.close()
        conn.close()
        print(f"{len(rows)}개의 데이터를 DB에서 성공적으로 불러왔습니다.")
        return pd.DataFrame(rows)
    except Exception as e:
        print(f"데이터베이스 연결 오류: {e}")
        return pd.DataFrame()

# --- 2. 구글 기반 로컬 모델 로드 --- ###
# ?? 수정된 부분 1: 다운로드한 구글 모델 폴더 경로를 지정합니다.
embedding_model_path = "C:/my_google_models/embedding_model"
llm_model_path = "C:/my_google_models/llm_model"

print(f"로컬 경로에서 임베딩 모델을 로드합니다: {embedding_model_path}")
embedding_model = SentenceTransformer(embedding_model_path)
print("임베딩 모델 로드 완료.")

# Qdrant 준비
qdrant_client = QdrantClient(":memory:")
qdrant_client.recreate_collection(
    collection_name="parts_db_collection",
    vectors_config=models.VectorParams(
        size=embedding_model.get_sentence_embedding_dimension(), # 모델에 맞는 벡터 사이즈 자동 설정
        distance=models.Distance.COSINE
    )
)

# --- 3. 데이터 주입 (?? embedding_model 특성 반영) ---
db_connection_info = {
    "host": "YOUR_DB_HOST", "dbname": "YOUR_DB_NAME",
    "user": "YOUR_DB_USER", "password": "YOUR_DB_PASSWORD"
}
parts_df = load_data_from_db(db_connection_info)

if not parts_df.empty:
    points_to_ingest = []
    for idx, row in parts_df.iterrows():
        text_to_embed = f"부품명: {row.get('part_name', '')}, 상태: {row.get('status_msg', '')}, 타입: {row.get('type', '')}"
        
        # ?? 수정된 부분 2: e5 모델은 검색할 문서 앞에 'passage: '를 붙여줘야 성능이 극대화됩니다.
        vector = embedding_model.encode('passage: ' + text_to_embed).tolist()
        
        payload = row.to_dict()
        points_to_ingest.append(models.PointStruct(id=idx, vector=vector, payload=payload))
        
    qdrant_client.upsert(collection_name="parts_db_collection", points=points_to_ingest, wait=True)
    print("DB 데이터를 Qdrant에 성공적으로 주입했습니다.")

# --- 4. Gemma LLM 로드 및 하이브리드 함수 --- ###
print(f"로컬 경로에서 Gemma LLM을 로드합니다: {llm_model_path}")
device = "cuda" if torch.cuda.is_available() else "cpu"

tokenizer = AutoTokenizer.from_pretrained(llm_model_path)
model = AutoModelForCausalLM.from_pretrained(
    llm_model_path,
    device_map="auto" # VRAM에 맞춰 자동으로 모델 레이어 분배
)

# 파이프라인 대신 tokenizer와 model을 직접 사용 (Gemma에 더 안정적)
# generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)
print("Gemma LLM 로드 완료.")

def ask_hybrid_system(question: str):
    # ... (부품 코드 추출 및 직접 조회 부분은 이전과 동일)
    part_code_pattern = r'[A-Z0-9]{4}-[A-Z0-9]{5,}'
    found_codes = re.findall(part_code_pattern, question)
    
    if found_codes:
        # ... (이전 코드와 동일)
        part_code = found_codes[0]
        print(f"부품 코드 '{part_code}' 직접 조회...")
        hits = qdrant_client.scroll(
            collection_name="parts_db_collection",
            scroll_filter=models.Filter(must=[models.FieldCondition(key="part_name", match=models.MatchValue(value=part_code))]),
            limit=1
        )
        return f"'{part_code}' 부품 정보:\n{hits[0][0].payload}" if hits[0] else f"'{part_code}' 정보를 찾을 수 없습니다."
    else:
        print("자연어 질문 RAG 검색...")
        # ?? 수정된 부분 3: e5 모델은 질문 앞에 'query: '를 붙여줘야 성능이 극대화됩니다.
        question_vector = embedding_model.encode('query: ' + question).tolist()
        
        hits = qdrant_client.search(collection_name="parts_db_collection", query_vector=question_vector, limit=1)
        if not hits: return "관련 정보를 찾을 수 없습니다."
        
        context = str(hits[0].payload)
        
        # ?? 수정된 부분 4: Gemma 모델이 가장 잘 알아듣는 프롬프트 형식으로 변경
        messages = [
            {"role": "user", "content": f"주어진 '정보'를 바탕으로 '질문'에 대해 한국어로 간결하게 답변하세요. 정보에 없는 내용은 절대로 언급하지 마세요.\n\n### 정보:\n{context}\n\n### 질문:\n{question}"}
        ]
        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        
        inputs = tokenizer.encode(prompt, add_special_tokens=True, return_tensors="pt").to(device)
        outputs = model.generate(input_ids=inputs, max_new_tokens=150)
        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # 프롬프트 부분을 답변에서 제거
        answer = answer.split('<start_of_turn>model')[-1].strip()
        return answer

# --- 5. 테스트 (동일) ---
print("\n--- RAG 시스템 테스트 시작 ---")
print(ask_hybrid_system("6HS2-00978A 부품의 전체 정보가 궁금해."))
print("\n---")
print(ask_hybrid_system("Parts Empty Stop을 사용하고 Set count가 Yes인 부품은 뭐야?