import pdfplumber
import re
import statistics
from typing import List, Dict, Any
from collections import defaultdict
import numpy as np

# ==============================================================================
# 데이터 구조 클래스 (변경 없음)
# ==============================================================================
class TextElement:
    def __init__(self, element: Dict[str, Any]):
        self.value = element.get('text', '')
        self.x0, self.y0, self.x1, self.y1 = [round(element.get(k, 0), 2) for k in ['x0', 'y0', 'x1', 'y1']]
        self.size = round(element.get('size', 0), 2)
        self.color = element.get('non_stroking_color', (0, 0, 0))

class PartComponent:
    def __init__(self, page_number: int, part_number_element: TextElement):
        self.page_number = page_number
        self.part_number = part_number_element
        self.related_elements: List[TextElement] = []

    def add_element(self, element: TextElement):
        self.related_elements.append(element)

    def get_details_string(self) -> str:
        if not self.related_elements: return "No other details found."
        sorted_elements = sorted(self.related_elements, key=lambda e: (e.y0, e.x0))
        details_list = [f"Color: {e.color}, Size: {e.size}, Value: '{e.value}', Position: (x0={e.x0}, y0={e.y0}, x1={e.x1}, y1={e.y1})" for e in sorted_elements]
        return ",\n\t".join(details_list)

# ==============================================================================
# ? 완전히 새로워진 메인 분석 함수 ?
# ==============================================================================
def analyze_parts_from_pdf(pdf_path: str, output_txt_path: str):
    all_part_components = []
    part_number_pattern = re.compile(r"^\d{4}-\d{6}$")

    print(f"PDF 분석 시작: {pdf_path}")
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages, 1):
                print(f"\n--- 페이지 {page_num} 처리 시작 ---")
                page_within_bounds = page.crop((0, 100, page.width, page.height))
                raw_words = page_within_bounds.extract_words(extra_attrs=["size", "non_stroking_color"])
                
                # 1. ? 데이터 정제: 좌표가 없거나 비정상적인 '유령 객체'를 완벽히 제거합니다.
                sanitized_words = [
                    w for w in raw_words
                    if w.get('y0', 0) > 100 and w.get('x1', 0) > w.get('x0', 0) and w.get('y1', 0) > w.get('y0', 0)
                ]
                
                if not sanitized_words:
                    print(f"페이지 {page_num}에서 유효한 텍스트를 찾지 못했습니다.")
                    continue
                
                print(f"원본 객체 {len(raw_words)}개 -> 유효 객체 {len(sanitized_words)}개로 정제 완료.")
                
                sorted_words = sorted(sanitized_words, key=lambda w: w['y0'])

                # 2. ? 완전 자동 그룹핑: 통계 분석으로 그룹 분리 기준을 자동 산출합니다.
                gaps = [
                    sorted_words[i+1]['y0'] - sorted_words[i]['y1']
                    for i in range(len(sorted_words) - 1)
                    if sorted_words[i+1]['y0'] > sorted_words[i]['y1']
                ]
                
                if not gaps: # 페이지에 텍스트 블록이 하나만 있는 경우
                    blocks = [sorted_words]
                else:
                    # 통계적 이상치(outlier)를 그룹 분리 기준으로 사용 (IQR 방식)
                    q1, q3 = np.percentile(gaps, [25, 75])
                    iqr = q3 - q1
                    # 일반적인 줄 간격보다 확연히 큰 간격을 자동으로 찾아냄
                    separation_threshold = q3 + 1.5 * iqr 
                    
                    print(f"페이지 {page_num}의 통계적 그룹 분리 임계값 자동 산출: {separation_threshold:.2f}px")

                    blocks, current_block = [], [sorted_words[0]]
                    for i in range(len(sorted_words) - 1):
                        gap = sorted_words[i+1]['y0'] - sorted_words[i]['y1']
                        if gap > separation_threshold:
                            blocks.append(current_block)
                            current_block = [sorted_words[i+1]]
                        else:
                            current_block.append(sorted_words[i+1])
                    blocks.append(current_block)
                
                print(f"페이지 {page_num}에서 {len(blocks)}개의 정보 블록을 감지했습니다.")

                # 3. 분리된 각 블록을 독립적으로 처리
                for block in blocks:
                    found_part_number_obj = next((w for w in block if part_number_pattern.match(w.get('text', ''))), None)
                    if found_part_number_obj:
                        part_element = TextElement(found_part_number_obj)
                        component = PartComponent(page_num, part_element)
                        for word_obj in block:
                            if word_obj is not found_part_number_obj:
                                component.add_element(TextElement(word_obj))
                        all_part_components.append(component)

        # 4. 페이지별로 재구성하여 출력 (이전과 동일)
        page_map = defaultdict(list)
        for component in all_part_components:
            page_map[component.page_number].append(component)

        with open(output_txt_path, 'w', encoding='utf-8') as f:
            f.write(f"총 {len(all_part_components)}개의 부품을 {len(page_map)}개 페이지에서 찾았습니다.\n")
            for page_num in sorted(page_map.keys()):
                components_on_page = page_map[page_num]
                f.write("\n" + "="*25 + f" PAGE {page_num} " + "="*25 + "\n")
                f.write(f"({len(components_on_page)}개의 부품 발견)\n\n")
                for i, component in enumerate(components_on_page, 1):
                    f.write(f"--- {i}. PartNumber: {component.part_number.value} ---\n")
                    f.write(f"Details: [\n\t{component.get_details_string()}\n]\n")
                    f.write("-" * 60 + "\n")

        print(f"\n분석 완료! 결과가 '{output_txt_path}' 파일에 저장되었습니다.")

    except Exception as e:
        print(f"스크립트 실행 중 오류가 발생했습니다: {e}")
        import traceback
        traceback.print_exc()

# --- 메인 실행 부분 ---
if __name__ == "__main__":
    # numpy 라이브러리가 필요합니다. 설치되어 있지 않다면: pip install numpy
    pdf_file_path = "YOUR_PDF_FILE_PATH.pdf" 
    output_file_path = "part_list_final_output.txt"
    analyze_parts_from_pdf(pdf_file_path, output_file_path)