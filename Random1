네, 알겠습니다. 직접 다운로드하는 방식, 즉 DB에서 직접 데이터를 가져와서 RAG 시스템을 구축하는 상세한 방법을 알려드리겠습니다.

두 번째 이미지에서 보여주신 psycopg2를 사용한 DB 연결 코드를 바탕으로 시작하겠습니다. 이 코드는 두 가지 기능을 모두 수행합니다.

직접 조회: 사용자가 질문에 부품 코드를 포함하면, Qdrant에서 해당 부품의 정보를 정확히 찾아 보여줍니다.

의미 검색 (RAG): 사용자가 자연어로 질문하면, 의미를 파악하여 가장 관련 있는 부품 정보를 찾아 LLM이 답변을 생성합니다.

## 0단계: 개발 환경 준비
기존 라이브러리에 DB 연결을 위한 psycopg2-binary를 추가로 설치합니다.

Bash

pip install qdrant-client pandas psycopg2-binary
pip install sentence-transformers
pip install transformers torch
## 1단계: DB에서 데이터 로드 ??
두 번째 이미지의 코드를 함수로 만들어, DB에서 데이터를 가져와 Pandas DataFrame 형태로 변환합니다. DataFrame은 데이터를 다루기 매우 편리합니다.

Python

import pandas as pd
import psycopg2
from psycopg2.extras import RealDictCursor

def load_data_from_db(conn_info: dict) -> pd.DataFrame:
    """PostgreSQL DB에 연결하여 parts_data 테이블의 모든 데이터를 DataFrame으로 가져옵니다."""
    try:
        print("데이터베이스에 연결합니다...")
        # conn_info 예시: {'host': 'localhost', 'dbname': 'mydb', 'user': 'user', 'password': 'password'}
        conn = psycopg2.connect(**conn_info)
        cursor = conn.cursor(cursor_factory=RealDictCursor) # 결과를 딕셔너리 형태로 받기 위함
        
        query = "SELECT * FROM parts_data;"
        cursor.execute(query)
        rows = cursor.fetchall()
        
        cursor.close()
        conn.close()
        
        print(f"{len(rows)}개의 데이터를 DB에서 성공적으로 불러왔습니다.")
        # 딕셔너리 리스트를 바로 DataFrame으로 변환
        return pd.DataFrame(rows)

    except Exception as e:
        print(f"데이터베이스 연결 또는 쿼리 실행 중 오류 발생: {e}")
        return pd.DataFrame() # 오류 발생 시 빈 DataFrame 반환
## 2단계: 모델 및 Qdrant 준비
이전과 동일하게 임베딩 모델과 Qdrant 클라이언트를 준비합니다.

Python

from qdrant_client import QdrantClient, models
from sentence_transformers import SentenceTransformer

# ... (이전 코드의 모델, Qdrant 클라이언트 설정 부분) ...
embedding_model = SentenceTransformer('BM-K/KoSimCSE-roberta-multitask')
qdrant_client = QdrantClient(":memory:")

# 컬렉션 생성 (DB 컬럼명을 보고 벡터 외에 저장할 필드를 예상)
qdrant_client.recreate_collection(
    collection_name="parts_db_collection",
    vectors_config=models.VectorParams(
        size=embedding_model.get_sentence_embedding_dimension(),
        distance=models.Distance.COSINE
    )
)
## 3단계: 데이터 주입 (DB 데이터 → Qdrant)
1단계에서 만든 함수로 DB 데이터를 가져온 뒤, Qdrant에 주입합니다.

Python

# DB 접속 정보 (실제 정보로 변경 필요)
db_connection_info = {
    "host": "YOUR_DB_HOST",
    "dbname": "YOUR_DB_NAME",
    "user": "YOUR_DB_USER",
    "password": "YOUR_DB_PASSWORD"
}

# DB에서 데이터 로드
parts_df = load_data_from_db(db_connection_info)

if not parts_df.empty:
    # Qdrant에 주입
    points_to_ingest = []
    # DataFrame의 각 행(row)을 처리
    for idx, row in parts_df.iterrows():
        # 첫 번째 이미지의 데이터 형식을 보고 중요한 텍스트 필드를 조합
        # 예시: 부품명, 상태 메시지 등을 조합하여 임베딩할 텍스트 생성
        text_to_embed = f"부품명: {row.get('part_name', '')}, 상태: {row.get('status_msg', '')}, 타입: {row.get('type', '')}"
        
        vector = embedding_model.encode(text_to_embed).tolist()
        payload = row.to_dict()
        
        points_to_ingest.append(models.PointStruct(id=idx, vector=vector, payload=payload))

    qdrant_client.upsert(
        collection_name="parts_db_collection",
        points=points_to_ingest,
        wait=True
    )
    print("DB 데이터를 Qdrant에 성공적으로 주입했습니다.")
## 4단계: 하이브리드 검색 및 답변 생성 함수
사용자 질문에서 부품 코드를 먼저 추출하고, 있으면 직접 조회, 없으면 의미 검색을 수행하는 하이브리드 함수를 만듭니다.

Python

import re
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch

# LLM 준비 (이전 코드와 동일)
model_name = 'yanado/KORani-v2-1.3B'
device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)

def ask_hybrid_system(question: str):
    # 1. 부품 코드 추출 (정규식 사용)
    # 예시 패턴: 4자리영문대문자 + 숫자 - 5자리영문대문자 + 숫자 (실제 패턴에 맞게 수정)
    part_code_pattern = r'[A-Z0-9]{4}-[A-Z0-9]{5,}'
    found_codes = re.findall(part_code_pattern, question)
    
    # 2. 부품 코드가 있으면 -> 직접 조회
    if found_codes:
        part_code = found_codes[0]
        print(f"부품 코드 '{part_code}'를 감지했습니다. 직접 조회를 수행합니다.")
        
        # Qdrant의 필터 기능으로 정확한 데이터 검색
        hits = qdrant_client.scroll(
            collection_name="parts_db_collection",
            scroll_filter=models.Filter(
                must=[
                    models.FieldCondition(
                        key="part_name", # DB의 부품 코드 컬럼명
                        match=models.MatchValue(value=part_code)
                    )
                ]
            ),
            limit=1
        )
        if hits[0]:
            # payload 전체를 보기 좋게 출력
            return f"'{part_code}' 부품 정보:\n{hits[0][0].payload}"
        else:
            return f"'{part_code}'에 대한 정보를 찾을 수 없습니다."

    # 3. 부품 코드가 없으면 -> 의미 검색 (RAG)
    else:
        print("자연어 질문으로 감지했습니다. RAG 검색을 수행합니다.")
        # 이전 코드의 RAG 함수 내용과 동일
        question_vector = embedding_model.encode(question).tolist()
        hits = qdrant_client.search(
            collection_name="parts_db_collection",
            query_vector=question_vector,
            limit=1
        )
        if not hits:
            return "관련 정보를 찾을 수 없습니다."

        context = str(hits[0].payload) # payload 전체를 문맥으로 사용
        prompt = f"""### 지시:
        주어진 '정보'를 바탕으로 '질문'에 대해 한국어로 간결하게 답변하세요. 정보에 없는 내용은 절대로 언급하지 마세요.

        ### 정보:
        {context}

        ### 질문:
        {question}

        ### 답변:
        """
        response = generator(prompt, max_new_tokens=150, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)
        answer = response[0]['generated_text'].replace(prompt, "").strip()
        return answer

# --- 테스트 ---
# 테스트 1: 부품 코드로 직접 조회
print("\n--- 테스트 1: 직접 조회 ---")
print(ask_hybrid_system("6HS2-00978A 부품의 전체 정보가 궁금해."))

# 테스트 2: 자연어로 의미 검색
print("\n--- 테스트 2: RAG 검색 ---")
print(ask_hybrid_system("Parts Empty Stop을 사용하고 Set count가 Yes인 부품은 뭐야?"))






내말은 임베딩모델을 직접 사이트가서 다운받아야한다고 아님 회사방화병ㄱ이나 설정 바꺼야되나 뭐어케해야 주피터에서 바로되지 지금은 우선 사이틍에서 받는설명+ 전체코드다시



생각하는 과정 표시
네, 맞습니다. 회사 방화벽(보안 정책) 때문에 코드 실행 중에 허깅페이스 사이트에서 모델을 자동으로 다운로드하지 못하는 것이 거의 확실합니다.

이 문제를 해결하기 위해 모델을 인터넷이 되는 곳에서 직접 다운로드한 후, 로컬에 저장된 그 파일을 사용하도록 코드를 수정해야 합니다. 설정 변경이나 방화벽 해제 요청보다 이 방법이 훨씬 간단하고 빠릅니다.

아래에 1) 모델 파일 다운로드 방법과 2) 로컬 파일을 사용하도록 수정한 전체 코드를 상세히 설명해 드리겠습니다.

## 1. 허깅페이스 사이트에서 모델 파일 직접 다운로드하기
인터넷 접속이 가능한 환경(집 컴퓨터 등)에서 아래 두 모델의 파일을 다운로드하여 USB나 다른 방법으로 Jupyer를 실행하는 환경으로 옮겨야 합니다.

1.1. 임베딩 모델 다운로드 (BM-K/KoSimCSE-roberta-multitask)
폴더 생성: 모델 파일을 담을 폴더를 미리 만듭니다.

예시: C:/my_local_models/embedding_model

다운로드 (Git 사용이 가장 편리):

PC에 Git이 설치되어 있어야 합니다.

명령 프롬프트(cmd)나 터미널을 열고 아래 명령어를 순서대로 실행합니다.

Bash

# Git LFS(대용량 파일 시스템) 설치 (처음 한 번만)
git lfs install

# 1번에서 만든 폴더로 이동
cd C:/my_local_models/embedding_model

# 허깅페이스 저장소 복제(clone)
git clone https://huggingface.co/BM-K/KoSimCSE-roberta-multitask .
마지막 명령어 끝에 .(점)을 꼭 찍어주세요. 현재 폴더에 파일들이 다운로드됩니다.

1.2. LLM(답변 생성) 모델 다운로드 (yanado/KORani-v2-1.3B)
폴더 생성: LLM 모델을 담을 별도의 폴더를 만듭니다.

예시: C:/my_local_models/llm_model

다운로드 (Git 사용):

명령 프롬프트(cmd)를 열고 아래 명령어를 실행합니다.

Bash

# 1번에서 만든 폴더로 이동
cd C:/my_local_models/llm_model

# 허깅페이스 저장소 복제(clone)
git clone https://huggingface.co/yanado/KORani-v2-1.3B .
이제 C:/my_local_models 폴더 안에 두 모델의 모든 파일이 저장되었습니다.

## 2. 로컬 모델을 사용하도록 수정한 전체 Python 코드
이제 파이썬 코드에서 인터넷 주소 대신 방금 다운로드한 로컬 폴더 경로를 알려주도록 수정합니다.

수정된 부분은 ### --- 로컬 모델 로드 --- ### 주석으로 표시했습니다.

Python

import pandas as pd
import psycopg2
from psycopg2.extras import RealDictCursor
from qdrant_client import QdrantClient, models
from sentence_transformers import SentenceTransformer
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch
import re

# --- 1. DB에서 데이터 로드 (이전과 동일) ---
def load_data_from_db(conn_info: dict) -> pd.DataFrame:
    try:
        print("데이터베이스에 연결합니다...")
        conn = psycopg2.connect(**conn_info)
        cursor = conn.cursor(cursor_factory=RealDictCursor)
        query = "SELECT * FROM parts_data;"
        cursor.execute(query)
        rows = cursor.fetchall()
        cursor.close()
        conn.close()
        print(f"{len(rows)}개의 데이터를 DB에서 성공적으로 불러왔습니다.")
        return pd.DataFrame(rows)
    except Exception as e:
        print(f"데이터베이스 연결 오류: {e}")
        return pd.DataFrame()

# --- 2. 로컬 모델 로드 --- ###
# ?? 수정된 부분 1: 인터넷 주소 대신 다운로드한 로컬 폴더 경로를 지정합니다.
embedding_model_path = "C:/my_local_models/embedding_model"
llm_model_path = "C:/my_local_models/llm_model"

print(f"로컬 경로에서 임베딩 모델을 로드합니다: {embedding_model_path}")
embedding_model = SentenceTransformer(embedding_model_path)
print("임베딩 모델 로드 완료.")

# Qdrant 준비
qdrant_client = QdrantClient(":memory:")
qdrant_client.recreate_collection(
    collection_name="parts_db_collection",
    vectors_config=models.VectorParams(
        size=embedding_model.get_sentence_embedding_dimension(),
        distance=models.Distance.COSINE
    )
)

# --- 3. 데이터 주입 (이전과 동일) ---
db_connection_info = {
    "host": "YOUR_DB_HOST", "dbname": "YOUR_DB_NAME",
    "user": "YOUR_DB_USER", "password": "YOUR_DB_PASSWORD"
}
parts_df = load_data_from_db(db_connection_info)

if not parts_df.empty:
    points_to_ingest = []
    for idx, row in parts_df.iterrows():
        text_to_embed = f"부품명: {row.get('part_name', '')}, 상태: {row.get('status_msg', '')}, 타입: {row.get('type', '')}"
        vector = embedding_model.encode(text_to_embed).tolist()
        payload = row.to_dict()
        points_to_ingest.append(models.PointStruct(id=idx, vector=vector, payload=payload))
    qdrant_client.upsert(collection_name="parts_db_collection", points=points_to_ingest, wait=True)
    print("DB 데이터를 Qdrant에 성공적으로 주입했습니다.")

# --- 4. 로컬 LLM 로드 및 하이브리드 함수 --- ###
print(f"로컬 경로에서 LLM을 로드합니다: {llm_model_path}")
device = "cuda" if torch.cuda.is_available() else "cpu"

# ?? 수정된 부분 2: 여기도 로컬 폴더 경로를 사용합니다.
tokenizer = AutoTokenizer.from_pretrained(llm_model_path)
model = AutoModelForCausalLM.from_pretrained(llm_model_path).to(device)

generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)
print("LLM 로드 완료.")

def ask_hybrid_system(question: str):
    part_code_pattern = r'[A-Z0-9]{4}-[A-Z0-9]{5,}'
    found_codes = re.findall(part_code_pattern, question)
    
    if found_codes:
        part_code = found_codes[0]
        print(f"부품 코드 '{part_code}' 직접 조회...")
        hits = qdrant_client.scroll(
            collection_name="parts_db_collection",
            scroll_filter=models.Filter(must=[models.FieldCondition(key="part_name", match=models.MatchValue(value=part_code))]),
            limit=1
        )
        return f"'{part_code}' 부품 정보:\n{hits[0][0].payload}" if hits[0] else f"'{part_code}' 정보를 찾을 수 없습니다."
    else:
        print("자연어 질문 RAG 검색...")
        question_vector = embedding_model.encode(question).tolist()
        hits = qdrant_client.search(collection_name="parts_db_collection", query_vector=question_vector, limit=1)
        if not hits: return "관련 정보를 찾을 수 없습니다."
        
        context = str(hits[0].payload)
        prompt = f"### 지시:\n주어진 '정보'를 바탕으로 '질문'에 대해 한국어로 간결하게 답변하세요. 정보에 없는 내용은 절대로 언급하지 마세요.\n\n### 정보:\n{context}\n\n### 질문:\n{question}\n\n### 답변:\n"
        response = generator(prompt, max_new_tokens=150, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)
        answer = response[0]['generated_text'].replace(prompt, "").strip()
        return answer

# --- 5. 테스트 (이전과 동일) ---
print("\n--- RAG 시스템 테스트 시작 ---")
print(ask_hybrid_system("6HS2-00978A 부품의 전체 정보가 궁금해."))
print("\n---")
print(ask_hybrid_system("Parts Empty Stop을 사용하고 Set count가 Yes인 부품은 뭐야?"))
이제 이 코드를 실행하면 인터넷에 접속하는 대신, 지정된 로컬 폴더에서 모델을 직접 불러오기 때문에 회사 방화벽에 막히지 않고 정상적으로 작동할 것입니다.