네, 알겠습니다. 직접 다운로드하는 방식, 즉 DB에서 직접 데이터를 가져와서 RAG 시스템을 구축하는 상세한 방법을 알려드리겠습니다.

두 번째 이미지에서 보여주신 psycopg2를 사용한 DB 연결 코드를 바탕으로 시작하겠습니다. 이 코드는 두 가지 기능을 모두 수행합니다.

직접 조회: 사용자가 질문에 부품 코드를 포함하면, Qdrant에서 해당 부품의 정보를 정확히 찾아 보여줍니다.

의미 검색 (RAG): 사용자가 자연어로 질문하면, 의미를 파악하여 가장 관련 있는 부품 정보를 찾아 LLM이 답변을 생성합니다.

## 0단계: 개발 환경 준비
기존 라이브러리에 DB 연결을 위한 psycopg2-binary를 추가로 설치합니다.

Bash

pip install qdrant-client pandas psycopg2-binary
pip install sentence-transformers
pip install transformers torch
## 1단계: DB에서 데이터 로드 ??
두 번째 이미지의 코드를 함수로 만들어, DB에서 데이터를 가져와 Pandas DataFrame 형태로 변환합니다. DataFrame은 데이터를 다루기 매우 편리합니다.

Python

import pandas as pd
import psycopg2
from psycopg2.extras import RealDictCursor

def load_data_from_db(conn_info: dict) -> pd.DataFrame:
    """PostgreSQL DB에 연결하여 parts_data 테이블의 모든 데이터를 DataFrame으로 가져옵니다."""
    try:
        print("데이터베이스에 연결합니다...")
        # conn_info 예시: {'host': 'localhost', 'dbname': 'mydb', 'user': 'user', 'password': 'password'}
        conn = psycopg2.connect(**conn_info)
        cursor = conn.cursor(cursor_factory=RealDictCursor) # 결과를 딕셔너리 형태로 받기 위함
        
        query = "SELECT * FROM parts_data;"
        cursor.execute(query)
        rows = cursor.fetchall()
        
        cursor.close()
        conn.close()
        
        print(f"{len(rows)}개의 데이터를 DB에서 성공적으로 불러왔습니다.")
        # 딕셔너리 리스트를 바로 DataFrame으로 변환
        return pd.DataFrame(rows)

    except Exception as e:
        print(f"데이터베이스 연결 또는 쿼리 실행 중 오류 발생: {e}")
        return pd.DataFrame() # 오류 발생 시 빈 DataFrame 반환
## 2단계: 모델 및 Qdrant 준비
이전과 동일하게 임베딩 모델과 Qdrant 클라이언트를 준비합니다.

Python

from qdrant_client import QdrantClient, models
from sentence_transformers import SentenceTransformer

# ... (이전 코드의 모델, Qdrant 클라이언트 설정 부분) ...
embedding_model = SentenceTransformer('BM-K/KoSimCSE-roberta-multitask')
qdrant_client = QdrantClient(":memory:")

# 컬렉션 생성 (DB 컬럼명을 보고 벡터 외에 저장할 필드를 예상)
qdrant_client.recreate_collection(
    collection_name="parts_db_collection",
    vectors_config=models.VectorParams(
        size=embedding_model.get_sentence_embedding_dimension(),
        distance=models.Distance.COSINE
    )
)
## 3단계: 데이터 주입 (DB 데이터 → Qdrant)
1단계에서 만든 함수로 DB 데이터를 가져온 뒤, Qdrant에 주입합니다.

Python

# DB 접속 정보 (실제 정보로 변경 필요)
db_connection_info = {
    "host": "YOUR_DB_HOST",
    "dbname": "YOUR_DB_NAME",
    "user": "YOUR_DB_USER",
    "password": "YOUR_DB_PASSWORD"
}

# DB에서 데이터 로드
parts_df = load_data_from_db(db_connection_info)

if not parts_df.empty:
    # Qdrant에 주입
    points_to_ingest = []
    # DataFrame의 각 행(row)을 처리
    for idx, row in parts_df.iterrows():
        # 첫 번째 이미지의 데이터 형식을 보고 중요한 텍스트 필드를 조합
        # 예시: 부품명, 상태 메시지 등을 조합하여 임베딩할 텍스트 생성
        text_to_embed = f"부품명: {row.get('part_name', '')}, 상태: {row.get('status_msg', '')}, 타입: {row.get('type', '')}"
        
        vector = embedding_model.encode(text_to_embed).tolist()
        payload = row.to_dict()
        
        points_to_ingest.append(models.PointStruct(id=idx, vector=vector, payload=payload))

    qdrant_client.upsert(
        collection_name="parts_db_collection",
        points=points_to_ingest,
        wait=True
    )
    print("DB 데이터를 Qdrant에 성공적으로 주입했습니다.")
## 4단계: 하이브리드 검색 및 답변 생성 함수
사용자 질문에서 부품 코드를 먼저 추출하고, 있으면 직접 조회, 없으면 의미 검색을 수행하는 하이브리드 함수를 만듭니다.

Python

import re
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch

# LLM 준비 (이전 코드와 동일)
model_name = 'yanado/KORani-v2-1.3B'
device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)

def ask_hybrid_system(question: str):
    # 1. 부품 코드 추출 (정규식 사용)
    # 예시 패턴: 4자리영문대문자 + 숫자 - 5자리영문대문자 + 숫자 (실제 패턴에 맞게 수정)
    part_code_pattern = r'[A-Z0-9]{4}-[A-Z0-9]{5,}'
    found_codes = re.findall(part_code_pattern, question)
    
    # 2. 부품 코드가 있으면 -> 직접 조회
    if found_codes:
        part_code = found_codes[0]
        print(f"부품 코드 '{part_code}'를 감지했습니다. 직접 조회를 수행합니다.")
        
        # Qdrant의 필터 기능으로 정확한 데이터 검색
        hits = qdrant_client.scroll(
            collection_name="parts_db_collection",
            scroll_filter=models.Filter(
                must=[
                    models.FieldCondition(
                        key="part_name", # DB의 부품 코드 컬럼명
                        match=models.MatchValue(value=part_code)
                    )
                ]
            ),
            limit=1
        )
        if hits[0]:
            # payload 전체를 보기 좋게 출력
            return f"'{part_code}' 부품 정보:\n{hits[0][0].payload}"
        else:
            return f"'{part_code}'에 대한 정보를 찾을 수 없습니다."

    # 3. 부품 코드가 없으면 -> 의미 검색 (RAG)
    else:
        print("자연어 질문으로 감지했습니다. RAG 검색을 수행합니다.")
        # 이전 코드의 RAG 함수 내용과 동일
        question_vector = embedding_model.encode(question).tolist()
        hits = qdrant_client.search(
            collection_name="parts_db_collection",
            query_vector=question_vector,
            limit=1
        )
        if not hits:
            return "관련 정보를 찾을 수 없습니다."

        context = str(hits[0].payload) # payload 전체를 문맥으로 사용
        prompt = f"""### 지시:
        주어진 '정보'를 바탕으로 '질문'에 대해 한국어로 간결하게 답변하세요. 정보에 없는 내용은 절대로 언급하지 마세요.

        ### 정보:
        {context}

        ### 질문:
        {question}

        ### 답변:
        """
        response = generator(prompt, max_new_tokens=150, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)
        answer = response[0]['generated_text'].replace(prompt, "").strip()
        return answer

# --- 테스트 ---
# 테스트 1: 부품 코드로 직접 조회
print("\n--- 테스트 1: 직접 조회 ---")
print(ask_hybrid_system("6HS2-00978A 부품의 전체 정보가 궁금해."))

# 테스트 2: 자연어로 의미 검색
print("\n--- 테스트 2: RAG 검색 ---")
print(ask_hybrid_system("Parts Empty Stop을 사용하고 Set count가 Yes인 부품은 뭐야?"))