베딩 후 검색(Retrieval) 설정을 맞추는 단계는 RAG 시스템의 성능을 결정하는 핵심 과정입니다. 가장 추천하는 조합부터 설명해 드릴게요.

결론부터 말하면, 하이브리드 검색을 기반으로 top_k를 5~10 사이로 설정하고, 리랭커(Re-ranker)를 추가하는 것이 현재 가장 성능이 좋은 표준적인 조합입니다.

## 1. 검색 방법: 렉시컬, 벡터, 하이브리드
어떤 검색 방식을 선택할지는 시스템의 성패를 좌우합니다.

렉시컬 검색 (Lexical Search, 예: BM25)

원리: 키워드 기반 검색. TF-IDF처럼 문서에서 단어의 빈도와 중요도를 계산해 정확히 일치하는 단어가 포함된 문서를 찾습니다.

장점: 모델 번호(NPU-1024), 에러 코드(Error 404), 약어, 고유명사처럼 정확한 키워드를 찾아내는 데 매우 강력합니다.

단점: "자동차"라고 검색하면 "차량"이 포함된 문서를 찾지 못하는 등 의미나 문맥을 전혀 이해하지 못합니다.

벡터 검색 (Vector Search)

원리: 의미 기반 검색. 문장의 의미를 나타내는 벡터(Embedding) 간의 유사도(거리)를 계산합니다.

장점: "자동차에 문제가 생겼어"라고 검색하면 "차량 수리 방법" 문서를 찾아주는 등 유사한 의미나 맥락을 가진 문서를 기가 막히게 찾아냅니다.

단점: NPU-1024 같은 특정 키워드가 포함된 문서를 놓칠 수 있습니다. 의미적으로는 다른 문서가 더 가깝다고 판단할 수 있기 때문입니다.

🏆 하이브리드 검색 (Hybrid Search)

원리: 렉시컬 검색과 벡터 검색을 결합하여 각 방식의 점수를 합산해 최종 순위를 매깁니다.

장점: 의미적으로도 관련성이 높으면서(벡터) 동시에 사용자가 입력한 특정 키워드(렉시컬)도 포함된 문서를 최우선으로 찾아줍니다. 두 방식의 단점을 서로 보완하여 가장 정확하고 안정적인 검색 결과를 제공합니다.

결론: 특별한 이유가 없다면 무조건 하이브리드 검색으로 시작하세요.

## 2. 주요 파라미터: top_k와 Threshold
검색 방식을 정했다면, 얼마나, 그리고 어떤 기준으로 문서를 가져올지 정해야 합니다.

top_k (가져올 문서의 개수)

역할: 유사도가 높은 순서대로 상위 k개의 문서를 가져오는 설정입니다. RAG 성능에 가장 직접적인 영향을 미칩니다.

설정 가이드:

너무 낮으면 (k=1~3): 정답이 포함된 문서를 아예 놓칠 위험이 커집니다 (재현율 저하).

너무 높으면 (k=20 이상): 정답과 관련 없는 노이즈 문서들이 LLM에 함께 입력되어, LLM이 혼란에 빠지고 답변의 정확도가 떨어집니다 (정확률 저하).

추천 값: 5에서 10 사이의 값으로 시작하는 것이 가장 일반적이고 효과적입니다. 답변에 필요한 정보를 충분히 포함하면서도 노이즈는 최소화하는 균형 잡힌 값입니다.

Similarity Score Threshold (유사도 점수 임계값)

역할: 검색된 문서의 유사도 점수가 설정한 임계값보다 낮으면 결과에서 제외하는 필터입니다. "최소한 이 정도는 관련 있어야 한다"는 기준선입니다.

설정 가이드:

코사인 유사도 기준 보통 0.7 ~ 0.8 사이로 설정하지만, 데이터의 특성에 따라 매우 민감하게 반응합니다.

추천: 처음에는 설정하지 않는 것을 권장합니다. top_k로 개수를 제한하는 것이 더 직관적이고 안정적입니다. top_k로 가져온 결과에 관련 없는 문서가 너무 많이 섞여 나올 때, 후순위로 튜닝하기 위해 사용하는 것이 좋습니다.

## 3. 성능 극대화: 리랭커 (Re-ranker)
리랭커는 RAG 시스템의 "치트키"와도 같습니다.

역할: 하이브리드 검색으로 가져온 top_k개의 문서들(예: 10개)을 더 정교하고 강력한 모델을 사용해 다시 한번 순위를 매기는 2차 필터입니다.

작동 방식:

빠른 속도로 일단 후보 문서 10개를 가져옵니다 (top_k=10).

속도는 조금 느리지만 훨씬 정확한 리랭커 모델이 이 10개 문서와 사용자 질문 사이의 관련성을 하나하나 다시 계산합니다.

가장 관련성 높은 순서로 10개 문서의 순서를 재배열합니다.

최종적으로 리랭킹된 순위에서 상위 1~3개만 LLM에 전달합니다.

장점: 초기 검색에서 순위가 약간 밀렸던 "진짜 정답" 문서를 맨 위로 끌어올려 줍니다. 최종 LLM에 입력되는 컨텍스트의 질을 극적으로 향상시킵니다.

추천: 성능 향상을 원한다면 반드시 도입하는 것을 고려하세요. Cohere Rerank 모델이나 Sentence-Transformers의 Cross-Encoder 모델들이 주로 사용됩니다.

## 📝 최종 추천 워크플로우
검색 방식: 하이브리드 검색을 채택합니다.

초기 검색: **top_k=10**으로 설정하여 관련성 있는 후보 문서를 넉넉하게 가져옵니다.

순위 재조정: 가져온 10개의 문서에 리랭커를 적용합니다.

최종 선택: 리랭킹된 결과에서 상위 3개의 문서를 선택하여 LLM에 최종 컨텍스트로 전달합니다.

테스트 및 튜닝: 실제 질문들을 테스트하며 top_k 값을 조절하고, 필요시 Threshold를 추가하여 시스템을 최적화합니다.






Deep Research
