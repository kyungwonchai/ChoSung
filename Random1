# 전체 실행 코드
아래 코드를 복사해서 main.py와 같은 파이썬 파일 하나에 모두 붙여넣고 실행하시면 됩니다.

Python

# ==============================================================================
# 1단계: 필요한 모든 도구 불러오기
# ==============================================================================
import os
import pandas as pd
from langchain.text_splitter import RecursiveCharacterTextSplitter
from qdrant_client import QdrantClient, models
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ==============================================================================
# 2단계: 설정 (사용자가 수정해야 할 부분)
# ==============================================================================
# ?? 학습시킬 로컬 리액트 프로젝트 폴더 경로를 지정합니다.
REACT_PROJECT_PATH = r"C:\path\to\your\react\project" 

# 다운로드한 로컬 모델들의 폴더 경로
EMBEDDING_MODEL_PATH = r"C:\my_google_models\embedding_model"
LLM_MODEL_PATH = r"C:\my_google_models\llm_model"

# ==============================================================================
# 3단계: 핵심 기능 함수 정의
# ==============================================================================

def load_files_from_react_project(project_path: str) -> dict:
    """지정된 React 프로젝트 폴더에서 소스 코드 파일들을 읽어 내용을 반환합니다."""
    print(f"'{project_path}' 폴더에서 소스 코드 파일을 읽습니다...")
    
    file_contents = {}
    target_extensions = ['.js', '.jsx', '.ts', '.tsx', '.md', '.txt']
    ignore_folders = ['node_modules', '.git', 'build', 'dist', 'public']

    for root, dirs, files in os.walk(project_path):
        dirs[:] = [d for d in dirs if d not in ignore_folders]
        
        for file in files:
            if any(file.endswith(ext) for ext in target_extensions):
                file_path = os.path.join(root, file)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        file_contents[file_path] = f.read()
                except Exception as e:
                    print(f"    - 오류: {file_path} 파일을 읽는 데 실패 - {e}")

    print(f"파일 읽기 완료. 총 {len(file_contents)}개의 소스 코드 파일을 수집했습니다.")
    return file_contents


def process_and_chunk(files: dict) -> list:
    """수집된 파일 내용들을 텍스트를 작은 조각(Chunk)으로 나눕니다."""
    print("데이터 정제 및 청킹(Chunking)을 시작합니다...")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100
    )
    
    all_chunks = []
    for path, content in files.items():
        chunks = text_splitter.split_text(content)
        for chunk in chunks:
            all_chunks.append({"text": chunk, "source": path})
            
    print(f"청킹 완료. 총 {len(all_chunks)}개의 텍스트 청크를 생성했습니다.")
    return all_chunks


def ask_codebase_expert(question: str, qdrant_cli, embed_model, gen_model, gen_tokenizer):
    """사용자 질문에 대해 로컬 코드베이스 내용을 기반으로 답변합니다."""
    print("\n--- 질문에 대한 답변 생성 중 ---")
    
    question_vector = embed_model.encode('query: ' + question).tolist()
    
    hits = qdrant_cli.search(
        collection_name="react_codebase",
        query_vector=question_vector,
        limit=3 # 답변 생성에 참고할 정보의 개수
    )
    if not hits:
        return "죄송합니다, 해당 질문에 대한 정보를 코드에서 찾을 수 없습니다."
    
    context = ""
    sources = set()
    for hit in hits:
        context += f"// 출처 파일: {hit.payload['source']}\n\n{hit.payload['text']}\n\n"
        sources.add(hit.payload['source'])
        
    messages = [
        {"role": "user", "content": f"당신은 우리 회사 React 프로젝트의 코드 전문가입니다. 아래 '참고 코드'만을 바탕으로 '질문'에 대해 한국어로 상세히 답변해주세요. 답변 마지막에는 출처 파일 경로를 명시해주세요.\n\n### 참고 코드:\n{context}\n\n### 질문:\n{question}"}
    ]
    prompt = gen_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    inputs = gen_tokenizer.encode(prompt, add_special_tokens=True, return_tensors="pt").to(gen_model.device)
    outputs = gen_model.generate(input_ids=inputs, max_new_tokens=512)
    answer = gen_tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    final_answer = answer.split('<start_of_turn>model')[-1].strip()
    final_answer += f"\n\n출처: {', '.join(sources)}"
    
    return final_answer

# ==============================================================================
# 4단계: 메인 실행 블록 (이 파일을 실행하면 여기서부터 시작됩니다)
# ==============================================================================
if __name__ == "__main__":
    # --- 1. 데이터 수집 및 처리 ---
    source_files = load_files_from_react_project(REACT_PROJECT_PATH)
    text_chunks = process_and_chunk(source_files)

    # --- 2. RAG 시스템 초기화 (모델 로딩) ---
    print("\n--- RAG 시스템 초기화 시작 ---")
    
    # 인터넷 접속을 막고 로컬 파일만 사용하도록 강제
    embedding_model = SentenceTransformer(EMBEDDING_MODEL_PATH, local_files_only=True)
    qdrant_client = QdrantClient(":memory:") # 매번 실행 시 새로 데이터를 주입하는 인메모리 방식

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"사용 장치: {device}")
    
    llm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_PATH, local_files_only=True)
    # VRAM에 맞게 모델을 올리고, 충돌을 피하기 위해 .to(device) 사용
    llm_model = AutoModelForCausalLM.from_pretrained(LLM_MODEL_PATH, local_files_only=True).to(device)
    
    print("--- 모든 모델 로드 완료 ---")

    # --- 3. Qdrant 벡터 DB에 데이터 주입 ---
    qdrant_client.recreate_collection(
        collection_name="react_codebase",
        vectors_config=models.VectorParams(size=embedding_model.get_sentence_embedding_dimension(), distance=models.Distance.COSINE)
    )
    
    points_to_ingest = []
    for i, chunk_data in enumerate(text_chunks):
        # e5 모델 규칙에 따라 'passage: ' 접두사 추가
        vector = embedding_model.encode('passage: ' + chunk_data['text']).tolist()
        points_to_ingest.append(models.PointStruct(id=i, vector=vector, payload=chunk_data))

    qdrant_client.upsert(collection_name="react_codebase", points=points_to_ingest, wait=True)
    print(f"--- {len(points_to_ingest)}개의 청크를 Qdrant에 주입 완료. 시스템 준비 완료 ---")

    # --- 4. 대화형으로 질문 시작 ---
    print("\n" + "="*50)
    print("이제부터 React 프로젝트에 대해 질문할 수 있습니다.")
    print("종료하려면 'exit' 또는 '종료'를 입력하세요.")
    print("="*50)

    while True:
        user_question = input("\n질문: ")
        if user_question.lower() in ['exit', '종료']:
            print("프로그램을 종료합니다.")
            break
        
        # 메인 함수를 호출하여 답변 생성
        response = ask_codebase_expert(user_question, qdrant_client, embedding_model, llm_model, llm_tokenizer)
        
        print(f"\n[답변]\n{response}")