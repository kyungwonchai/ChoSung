 모델 정의 (MLP + 정규화 + 드롭아웃 적용)
python
Copy code
import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data_utils

# 📌 MLP 기반 모델 정의 (정규화 + 드롭아웃 적용)
class QRMLPModel(nn.Module):
    def __init__(self, input_size, num_classes, dropout_rate=0.3):
        super(QRMLPModel, self).__init__()
        self.fc1 = nn.Linear(input_size, 128)
        self.bn1 = nn.BatchNorm1d(128)  # 배치 정규화 추가
        self.dropout1 = nn.Dropout(dropout_rate)  # 드롭아웃 추가

        self.fc2 = nn.Linear(128, 64)
        self.bn2 = nn.BatchNorm1d(64)
        self.dropout2 = nn.Dropout(dropout_rate)

        self.fc3 = nn.Linear(64, 32)
        self.bn3 = nn.BatchNorm1d(32)
        self.dropout3 = nn.Dropout(dropout_rate)

        self.fc4 = nn.Linear(32, num_classes)  # 출력 레이어
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.bn1(self.fc1(x)))
        x = self.dropout1(x)
        x = self.relu(self.bn2(self.fc2(x)))
        x = self.dropout2(x)
        x = self.relu(self.bn3(self.fc3(x)))
        x = self.dropout3(x)
        x = self.fc4(x)  # 최종 출력
        return x
✅ 훈련 함수 (L2 정규화 + 드롭아웃 적용)
python
Copy code
def train_qr_mlp_model(X_train, y_train, X_val, y_val, input_size, num_classes, 
                         dropout_rate=0.3, weight_decay=1e-4, 
                         epochs=20, batch_size=64, learning_rate=0.001):

    # 🔹 데이터 로드
    train_dataset = data_utils.TensorDataset(torch.tensor(X_train, dtype=torch.float32), 
                                             torch.tensor(y_train, dtype=torch.long))
    val_dataset = data_utils.TensorDataset(torch.tensor(X_val, dtype=torch.float32), 
                                           torch.tensor(y_val, dtype=torch.long))
    train_loader = data_utils.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = data_utils.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    # 🔹 모델 초기화 (정규화 + 드롭아웃 포함)
    model = QRMLPModel(input_size=input_size, num_classes=num_classes, dropout_rate=dropout_rate).to(device)

    criterion = nn.CrossEntropyLoss()  # 🔹 손실 함수
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)  # 🔹 L2 정규화 적용

    # 🔹 훈련 시작
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)

        # 🔹 검증 평가
        model.eval()
        val_loss = 0
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, targets in val_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = model(inputs)
                val_loss += criterion(outputs, targets).item()
                predicted = torch.argmax(outputs, dim=1)
                correct += (predicted == targets).sum().item()
                total += targets.size(0)

        val_loss /= len(val_loader)
        accuracy = correct / total

        print(f"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}")

    print("🎉 훈련 완료!")
    return model
📌 적용된 핵심 기술
✅ L2 정규화 (weight_decay=1e-4) → 모델의 가중치가 너무 커지는 것을 방지하여 과적합 줄임
✅ Dropout (nn.Dropout(0.3)) → 특정 뉴런을 랜덤하게 비활성화하여 특정 패턴에 과적합하는 것 방지
✅ Batch Normalization (nn.BatchNorm1d) → 학습 안정화 및 훈련 속도 향상
✅ Adam 옵티마이저 (optim.Adam) → weight_decay 추가하여 L2 정규화 적용
✅ 훈련/검증 Loss & Accuracy 출력 → 과적합 감지 가능