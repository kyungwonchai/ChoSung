rning) 없이도 기존 지식을 유지하면서 추가 학습을 효과적으로 수행할 수 있는 방법은 **Boosting 계열 알고리즘 (XGBoost, LightGBM, CatBoost)**이다.
또한 **하이퍼파라미터 자동 튜닝(AutoML)과 분할 학습(Split Learning)**을 적용하여 최적의 모델을 만들겠다.

📌 적용 기술
✅ Boosting 기반 모델 (LightGBM) → 데이터가 많을 때 강력한 성능
✅ Optuna로 하이퍼파라미터 자동 튜닝
✅ 데이터를 자동으로 훈련/검증/테스트로 분할
✅ 증분 학습이 필요 없도록 새로운 데이터만 추가 학습

✅ 최적화된 최신 머신러닝 모델 (LightGBM + Optuna)
python
Copy code
import lightgbm as lgb
import optuna
import numpy as np
import pandas as pd
import pymssql
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import joblib  # 모델 저장 및 로드용

# 📌 MSSQL에서 데이터 로드 (3개월 이상 지난 데이터 제외)
def load_data_from_mssql():
    conn = pymssql.connect(server="your_server", user="your_user", password="your_password", database="your_db")
    query = """
    SELECT Model, QR, Date
    FROM ModelQRTable
    WHERE Date >= DATEADD(MONTH, -3, GETDATE())  -- 3개월 이상 지난 데이터 제외
    """
    df = pd.read_sql(query, conn)
    conn.close()
    df["Date"] = pd.to_datetime(df["Date"], errors="coerce")  # 날짜 변환
    return df.drop(columns=["Date"])

# 📌 QR 값을 숫자 벡터로 변환 (문자를 ASCII 코드로 변환 후 패딩)
def vectorize_qr(qr_values, max_length):
    vectorized = np.zeros((len(qr_values), max_length), dtype=np.int32)
    for i, qr in enumerate(qr_values):
        for j, char in enumerate(qr[:max_length]):  # max_length 초과 시 자름
            vectorized[i, j] = ord(char)
    return vectorized

# 📌 LightGBM 모델 훈련
def train_lightgbm(df_data):
    qr_values = df_data["QR"].values
    model_names = df_data["Model"].values

    # 🔹 Label Encoding (문자를 숫자로 변환)
    encoder = LabelEncoder()
    y_encoded = encoder.fit_transform(model_names)

    # 🔹 QR 코드를 벡터화
    max_qr_length = max(len(qr) for qr in qr_values)
    X_vectorized = vectorize_qr(qr_values, max_qr_length)

    # 🔹 데이터 분할 (학습: 70%, 검증: 15%, 테스트: 15%)
    X_train, X_temp, y_train, y_temp = train_test_split(X_vectorized, y_encoded, test_size=0.3, random_state=42)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

    # 🔹 LightGBM 데이터셋 생성
    train_data = lgb.Dataset(X_train, label=y_train)
    val_data = lgb.Dataset(X_val, label=y_val)

    # 🔹 하이퍼파라미터 자동 튜닝 (Optuna)
    def objective(trial):
        params = {
            "objective": "multiclass",
            "num_class": len(encoder.classes_),  # 모델 개수
            "boosting_type": "gbdt",
            "metric": "multi_logloss",
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
            "num_leaves": trial.suggest_int("num_leaves", 20, 150),
            "max_depth": trial.suggest_int("max_depth", 3, 12),
            "feature_fraction": trial.suggest_float("feature_fraction", 0.6, 1.0),
            "bagging_fraction": trial.suggest_float("bagging_fraction", 0.6, 1.0),
            "bagging_freq": trial.suggest_int("bagging_freq", 1, 5),
            "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 10, 100),
        }
        model = lgb.train(params, train_data, valid_sets=[val_data], early_stopping_rounds=20, verbose_eval=False)
        preds = np.argmax(model.predict(X_val), axis=1)
        return accuracy_score(y_val, preds)

    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=50)  # 50번 반복하여 최적화

    best_params = study.best_params
    print(f"🔍 최적 파라미터: {best_params}")

    # 🔹 최적의 파라미터로 LightGBM 훈련
    final_model = lgb.train(best_params, train_data, valid_sets=[val_data], early_stopping_rounds=20, verbose_eval=10)

    # 🔹 모델 저장
    joblib.dump(final_model, "lightgbm_model.pkl")
    joblib.dump(encoder, "label_encoder.pkl")

    # 🔹 테스트 데이터 평가
    preds_test = np.argmax(final_model.predict(X_test), axis=1)
    accuracy = accuracy_score(y_test, preds_test)
    print(f"✅ 테스트 정확도: {accuracy:.4f}")

# 📌 QR 코드 예측
def predict_qr(qr_value):
    # 🔹 모델 및 인코더 불러오기
    model = joblib.load("lightgbm_model.pkl")
    encoder = joblib.load("label_encoder.pkl")

    max_qr_length = 10  # 기존 모델에 맞는 길이로 설정
    X_vectorized = vectorize_qr([qr_value], max_qr_length)

    # 🔹 예측 수행
    preds = np.argmax(model.predict(X_vectorized), axis=1)
    predicted_model = encoder.inverse_transform(preds)[0]

    print(f"✅ 예측된 모델: {predicted_model}")

# 📌 실행 예제
if __name__ == "__main__":
    df_data = load_data_from_mssql()  # 🔥 최신 데이터 가져오기
    train_lightgbm(df_data)  # LightGBM 학습
    test_qr = "123ABC456"  # 예측할 QR 값
    predict_qr(test_qr)  # 예측 실행
📌 이 방식이 좋은 이유
✅ Boosting 사용 (LightGBM) → 복잡한 데이터 패턴을 빠르고 정확하게 학습
✅ 하이퍼파라미터 자동 튜닝 (Optuna) → 최적의 모델을 자동으로 찾음
✅ 자동 데이터 분할 → 학습 / 검증 / 테스트 데이터 관리 최적화
✅ 기존 데이터 전체를 다시 학습하지 않고, 새로운 데이터만 추가 학습
✅ 메모리와 계산량 절약 (기존 모델을 덮어쓰지 않고 유지 가능)

📌 이 방법이 현실적으로 가장 좋은 이유
딥러닝(LSTM)보다 LightGBM이 더 빠르고, 적은 데이터로도 학습 가능
하이퍼파라미터 튜닝을 자동으로 해서 추가 학습 시 모델이 최적화됨