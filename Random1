1. train.py (훈련 코드)
python
복사
편집
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import pickle
import re

# 하이퍼파라미터 설정
EMBEDDING_DIM = 100
HIDDEN_DIM = 128
BATCH_SIZE = 32
LEARNING_RATE = 0.001
EARLY_STOPPING_ROUNDS = 5
TARGET_ACCURACY = 0.90
MODEL_PATH = "model.pth"

# ?? 데이터셋 클래스 정의
class TextDataset(Dataset):
    def __init__(self, file_paths, labels, vocab=None):
        self.file_paths = file_paths
        self.labels = labels
        self.vocab = vocab if vocab else self.build_vocab()
        self.label_encoder = LabelEncoder()
        self.encoded_labels = self.label_encoder.fit_transform(labels)

    def build_vocab(self):
        words = set()
        for file_path in self.file_paths:
            with open(file_path, 'r', encoding='utf-8') as f:
                words.update(self.tokenize(f.read()))
        vocab = {word: idx + 1 for idx, word in enumerate(words)}
        vocab['<PAD>'] = 0
        return vocab

    def tokenize(self, text):
        return re.sub(r'\W+', ' ', text).lower().split()

    def encode_text(self, text):
        tokens = self.tokenize(text)
        return [self.vocab.get(token, 0) for token in tokens]

    def __len__(self):
        return len(self.file_paths)

    def __getitem__(self, idx):
        file_path = self.file_paths[idx]
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
        encoded_text = self.encode_text(text)
        label = self.encoded_labels[idx]
        return torch.tensor(encoded_text, dtype=torch.long), torch.tensor(label, dtype=torch.long)

# ?? 모델 정의
class TextClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super(TextClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True, dropout=0.3)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, text):
        embedded = self.embedding(text)
        _, (hidden, _) = self.lstm(embedded)
        return self.fc(hidden[-1])

# ?? 데이터 로드 함수
def load_data(data_dir):
    file_paths, labels = [], []
    for file in os.listdir(data_dir):
        if file.endswith(".txt"):
            file_paths.append(os.path.join(data_dir, file))
            labels.append(os.path.splitext(file)[0])  # 파일명(숫자)을 정답으로 사용
    return file_paths, labels

# ?? 모델 학습
def train_model():
    train_dir = "data1/train"

    # 데이터 로드 및 분할
    file_paths, labels = load_data(train_dir)
    train_paths, val_paths, train_labels, val_labels = train_test_split(file_paths, labels, test_size=0.2, random_state=42)

    train_dataset = TextDataset(train_paths, train_labels)
    val_dataset = TextDataset(val_paths, val_labels, vocab=train_dataset.vocab)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = TextClassifier(len(train_dataset.vocab), EMBEDDING_DIM, HIDDEN_DIM, len(set(labels)))
    model.to(device)

    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.CrossEntropyLoss()

    best_acc = 0
    no_improve_count = 0

    for epoch in range(50):
        model.train()
        for texts, labels in train_loader:
            texts, labels = texts.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(texts)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for texts, labels in val_loader:
                texts, labels = texts.to(device), labels.to(device)
                outputs = model(texts)
                predicted = torch.argmax(outputs, dim=1)
                correct += (predicted == labels).sum().item()
                total += labels.size(0)

        val_acc = correct / total
        print(f"Epoch {epoch + 1}: Validation Accuracy = {val_acc:.4f}")

        if val_acc > best_acc:
            best_acc = val_acc
            no_improve_count = 0
            torch.save(model.state_dict(), MODEL_PATH)
            with open("vocab.pkl", "wb") as f:
                pickle.dump(train_dataset.vocab, f)
            with open("label_encoder.pkl", "wb") as f:
                pickle.dump(train_dataset.label_encoder, f)
            print(f"? 모델 저장 완료! (Accuracy: {val_acc:.4f})")
        else:
            no_improve_count += 1

        if val_acc >= TARGET_ACCURACY or no_improve_count >= EARLY_STOPPING_ROUNDS:
            print("? 학습 종료 (조기 종료 또는 목표 도달)")
            break

if __name__ == "__main__":
    train_model()
?? 2. predict.py (예측 코드)
python
복사
편집
import os
import torch
import pickle
import pandas as pd
from train import TextClassifier, MODEL_PATH

# ?? 예측을 위한 데이터셋 클래스
class PredictionDataset:
    def __init__(self, vocab):
        self.vocab = vocab

    def tokenize(self, text):
        return text.lower().split()

    def encode_text(self, text):
        tokens = self.tokenize(text)
        return [self.vocab.get(token, 0) for token in tokens]

# ?? 예측 수행 함수
def predict_from_folder(folder):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # ?? 모델 불러오기
    with open("vocab.pkl", "rb") as f:
        vocab = pickle.load(f)
    with open("label_encoder.pkl", "rb") as f:
        label_encoder = pickle.load(f)

    model = TextClassifier(len(vocab), 100, 128, len(label_encoder.classes_))
    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    model.to(device)
    model.eval()

    file_paths = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(".txt")]
    predictions = []

    with torch.no_grad():
        dataset = PredictionDataset(vocab)
        for file_path in file_paths:
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()
            input_tensor = torch.tensor([dataset.encode_text(text)], dtype=torch.long).to(device)
            output = model(input_tensor)
            pred_label = label_encoder.inverse_transform([torch.argmax(output, dim=1).item()])[0]
            predictions.append((os.path.basename(file_path), pred_label))

    df = pd.DataFrame(predictions, columns=["파일명", "예측된 숫자"])
    df.to_csv(os.path.join(folder, "prediction_results.txt"), index=False, sep="\t", encoding="utf-8")
    print(df)

if __name__ == "__main__":
    predict_from_folder("data3/test3")
?? 이제 train.py로 학습하고, predict.py에서 원하는 폴더(data3/test3/)를 예측하면 된다! ??????