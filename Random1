def train_model(new_df_data, epochs=10, batch_size=64):
    new_df_data = filter_recent_data(new_df_data)  # ğŸ”¥ 3ê°œì›” ì§€ë‚œ ë°ì´í„° ì œì™¸

    # ê¸°ì¡´ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    model, encoder, scaler, max_qr_length = load_model()

    if model is None:
        print("ğŸ”´ ê¸°ì¡´ ëª¨ë¸ ì—†ìŒ. ìƒˆë¡œ í•™ìŠµ ì‹œì‘.")
        encoder = LabelEncoder()
        full_df_data = new_df_data  # ìƒˆ ë°ì´í„°ë§Œ í•™ìŠµ
    else:
        print("ğŸŸ¢ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œë¨. ê¸°ì¡´ + ì¶”ê°€ í•™ìŠµ ì§„í–‰.")
        old_df_data = load_data_from_mssql()  # ğŸ”¥ ê¸°ì¡´ ë°ì´í„°ë„ ê°€ì ¸ì™€ì„œ ìœ ì§€
        full_df_data = pd.concat([old_df_data, new_df_data]).drop_duplicates().reset_index(drop=True)

    # QR ê°’, ëª¨ë¸ëª… ì¶”ì¶œ
    qr_values = full_df_data["QR"].values
    model_names = full_df_data["Model"].values

    # âœ… ê¸°ì¡´ encoder ìœ ì§€í•˜ë©´ì„œ ìƒˆë¡œìš´ ë°ì´í„°ë„ ë°˜ì˜
    if model is None:
        encoder.fit(model_names)
    else:
        encoder.classes_ = np.concatenate((encoder.classes_, np.setdiff1d(model_names, encoder.classes_)))

    y_encoded = encoder.transform(model_names)

    max_qr_length = max(len(qr) for qr in qr_values)
    X_vectorized = vectorize_qr(qr_values, max_qr_length)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_vectorized)

    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)

    train_dataset = data_utils.TensorDataset(torch.tensor(X_train, dtype=torch.float32),
                                             torch.tensor(y_train, dtype=torch.long))
    train_loader = data_utils.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    # âœ… ê¸°ì¡´ ëª¨ë¸ ìœ ì§€í•˜ë©´ì„œ, ì¶œë ¥ ë ˆì´ì–´ í¬ê¸°ë§Œ ì—…ë°ì´íŠ¸
    num_classes = len(encoder.classes_)
    if model is None:
        model = QRModel(X_train.shape[1], num_classes).to(device)
    elif model.fc4.out_features != num_classes:
        print(f"ğŸ”„ ì¶œë ¥ ë ˆì´ì–´ í¬ê¸° ë³€ê²½: {model.fc4.out_features} â†’ {num_classes}")
        model.fc4 = nn.Linear(32, num_classes).to(device)  # ìƒˆë¡œìš´ í´ë˜ìŠ¤ ì¶”ê°€ ë°˜ì˜

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # ğŸ”¥ ê¸°ì¡´ ë°ì´í„° ìœ ì§€í•œ ì±„ ì¶”ê°€ í•™ìŠµ
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        print(f"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}")

    save_model(model, encoder, scaler, max_qr_length)