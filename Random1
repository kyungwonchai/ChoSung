import json

def route_question(question: str, llm_generator, llm_tokenizer) -> dict:
    """
    사용자 질문의 의도를 파악하여 어떤 작업을 수행할지 결정(라우팅)합니다.
    LLM이 상상력을 발휘하지 못하도록 매우 구체적이고 엄격한 규칙을 제시합니다.
    """
    
    # LLM에게 역할과 따라야 할 규칙을 명확하게 제시하는 프롬프트
    prompt = f"""당신은 사용자 질문을 분석하여 JSON 형식으로 작업 계획을 세우는 '지능형 라우터'입니다.
    아래 규칙을 반드시 따르세요.

    [규칙]
    1. 질문에서 하이픈(-)이 있거나 없는 'XXXX-XXXXXX' 형식의 부품 코드가 명확히 보이는지 확인합니다.
    2. 부품 코드가 보이면, 반드시 "intent"를 "direct_lookup"으로 설정하고, 질문에 보이는 코드 문자열을 그대로 "part_code" 값으로 추출하세요.
    3. 부품 코드가 명확히 보이지 않으면, "intent"를 "rag_search"로 설정하세요. "part_code" 필드는 절대 포함하지 마세요.
    4. 단순 인사나 관련 없는 대화는 "intent"를 "general_talk"으로 설정하세요.

    [질문]
    {question}

    [JSON 결정]
    """
    
    # LLM을 사용하여 추론 실행
    # pad_token_id를 추가하여 생성 길이에 대한 경고 메시지를 줄입니다.
    inputs = llm_tokenizer.encode(prompt, add_special_tokens=False, return_tensors="pt").to(llm_generator.device)
    outputs = llm_generator.generate(input_ids=inputs, max_new_tokens=50, pad_token_id=llm_tokenizer.eos_token_id)
    response_text = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # LLM의 답변(텍스트)에서 JSON 부분만 안정적으로 추출
    # "```json" 과 "```" 사이의 내용을 찾는 방식을 추가하여 안정성 향상
    try:
        if "```json" in response_text:
            json_part = response_text.split("```json")[1].split("```")[0].strip()
        else:
            json_part = response_text.split("[JSON 결정]")[-1].strip()
        
        # 텍스트를 실제 JSON 객체(딕셔너리)로 변환
        decision = json.loads(json_part)
        return decision
    except (json.JSONDecodeError, IndexError):
        # LLM이 JSON 형식을 제대로 만들지 못하거나 추출에 실패한 경우,
        # 안전하게 RAG 검색으로 기본 처리합니다.
        print(f"경고: LLM의 응답에서 JSON을 파싱하는 데 실패했습니다. 응답: {response_text}")
        return {"intent": "rag_search"}

