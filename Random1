rning) ì—†ì´ë„ ê¸°ì¡´ ì§€ì‹ì„ ìœ ì§€í•˜ë©´ì„œ ì¶”ê°€ í•™ìŠµì„ íš¨ê³¼ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì€ **Boosting ê³„ì—´ ì•Œê³ ë¦¬ì¦˜ (XGBoost, LightGBM, CatBoost)**ì´ë‹¤.
ë˜í•œ **í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ íŠœë‹(AutoML)ê³¼ ë¶„í•  í•™ìŠµ(Split Learning)**ì„ ì ìš©í•˜ì—¬ ìµœì ì˜ ëª¨ë¸ì„ ë§Œë“¤ê² ë‹¤.

ğŸ“Œ ì ìš© ê¸°ìˆ 
âœ… Boosting ê¸°ë°˜ ëª¨ë¸ (LightGBM) â†’ ë°ì´í„°ê°€ ë§ì„ ë•Œ ê°•ë ¥í•œ ì„±ëŠ¥
âœ… Optunaë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ íŠœë‹
âœ… ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ë¡œ ë¶„í• 
âœ… ì¦ë¶„ í•™ìŠµì´ í•„ìš” ì—†ë„ë¡ ìƒˆë¡œìš´ ë°ì´í„°ë§Œ ì¶”ê°€ í•™ìŠµ

âœ… ìµœì í™”ëœ ìµœì‹  ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ (LightGBM + Optuna)
python
Copy code
import lightgbm as lgb
import optuna
import numpy as np
import pandas as pd
import pymssql
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import joblib  # ëª¨ë¸ ì €ì¥ ë° ë¡œë“œìš©

# ğŸ“Œ MSSQLì—ì„œ ë°ì´í„° ë¡œë“œ (3ê°œì›” ì´ìƒ ì§€ë‚œ ë°ì´í„° ì œì™¸)
def load_data_from_mssql():
    conn = pymssql.connect(server="your_server", user="your_user", password="your_password", database="your_db")
    query = """
    SELECT Model, QR, Date
    FROM ModelQRTable
    WHERE Date >= DATEADD(MONTH, -3, GETDATE())  -- 3ê°œì›” ì´ìƒ ì§€ë‚œ ë°ì´í„° ì œì™¸
    """
    df = pd.read_sql(query, conn)
    conn.close()
    df["Date"] = pd.to_datetime(df["Date"], errors="coerce")  # ë‚ ì§œ ë³€í™˜
    return df.drop(columns=["Date"])

# ğŸ“Œ QR ê°’ì„ ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜ (ë¬¸ìë¥¼ ASCII ì½”ë“œë¡œ ë³€í™˜ í›„ íŒ¨ë”©)
def vectorize_qr(qr_values, max_length):
    vectorized = np.zeros((len(qr_values), max_length), dtype=np.int32)
    for i, qr in enumerate(qr_values):
        for j, char in enumerate(qr[:max_length]):  # max_length ì´ˆê³¼ ì‹œ ìë¦„
            vectorized[i, j] = ord(char)
    return vectorized

# ğŸ“Œ LightGBM ëª¨ë¸ í›ˆë ¨
def train_lightgbm(df_data):
    qr_values = df_data["QR"].values
    model_names = df_data["Model"].values

    # ğŸ”¹ Label Encoding (ë¬¸ìë¥¼ ìˆ«ìë¡œ ë³€í™˜)
    encoder = LabelEncoder()
    y_encoded = encoder.fit_transform(model_names)

    # ğŸ”¹ QR ì½”ë“œë¥¼ ë²¡í„°í™”
    max_qr_length = max(len(qr) for qr in qr_values)
    X_vectorized = vectorize_qr(qr_values, max_qr_length)

    # ğŸ”¹ ë°ì´í„° ë¶„í•  (í•™ìŠµ: 70%, ê²€ì¦: 15%, í…ŒìŠ¤íŠ¸: 15%)
    X_train, X_temp, y_train, y_temp = train_test_split(X_vectorized, y_encoded, test_size=0.3, random_state=42)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

    # ğŸ”¹ LightGBM ë°ì´í„°ì…‹ ìƒì„±
    train_data = lgb.Dataset(X_train, label=y_train)
    val_data = lgb.Dataset(X_val, label=y_val)

    # ğŸ”¹ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ íŠœë‹ (Optuna)
    def objective(trial):
        params = {
            "objective": "multiclass",
            "num_class": len(encoder.classes_),  # ëª¨ë¸ ê°œìˆ˜
            "boosting_type": "gbdt",
            "metric": "multi_logloss",
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
            "num_leaves": trial.suggest_int("num_leaves", 20, 150),
            "max_depth": trial.suggest_int("max_depth", 3, 12),
            "feature_fraction": trial.suggest_float("feature_fraction", 0.6, 1.0),
            "bagging_fraction": trial.suggest_float("bagging_fraction", 0.6, 1.0),
            "bagging_freq": trial.suggest_int("bagging_freq", 1, 5),
            "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 10, 100),
        }
        model = lgb.train(params, train_data, valid_sets=[val_data], early_stopping_rounds=20, verbose_eval=False)
        preds = np.argmax(model.predict(X_val), axis=1)
        return accuracy_score(y_val, preds)

    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=50)  # 50ë²ˆ ë°˜ë³µí•˜ì—¬ ìµœì í™”

    best_params = study.best_params
    print(f"ğŸ” ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")

    # ğŸ”¹ ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¡œ LightGBM í›ˆë ¨
    final_model = lgb.train(best_params, train_data, valid_sets=[val_data], early_stopping_rounds=20, verbose_eval=10)

    # ğŸ”¹ ëª¨ë¸ ì €ì¥
    joblib.dump(final_model, "lightgbm_model.pkl")
    joblib.dump(encoder, "label_encoder.pkl")

    # ğŸ”¹ í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€
    preds_test = np.argmax(final_model.predict(X_test), axis=1)
    accuracy = accuracy_score(y_test, preds_test)
    print(f"âœ… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy:.4f}")

# ğŸ“Œ QR ì½”ë“œ ì˜ˆì¸¡
def predict_qr(qr_value):
    # ğŸ”¹ ëª¨ë¸ ë° ì¸ì½”ë” ë¶ˆëŸ¬ì˜¤ê¸°
    model = joblib.load("lightgbm_model.pkl")
    encoder = joblib.load("label_encoder.pkl")

    max_qr_length = 10  # ê¸°ì¡´ ëª¨ë¸ì— ë§ëŠ” ê¸¸ì´ë¡œ ì„¤ì •
    X_vectorized = vectorize_qr([qr_value], max_qr_length)

    # ğŸ”¹ ì˜ˆì¸¡ ìˆ˜í–‰
    preds = np.argmax(model.predict(X_vectorized), axis=1)
    predicted_model = encoder.inverse_transform(preds)[0]

    print(f"âœ… ì˜ˆì¸¡ëœ ëª¨ë¸: {predicted_model}")

# ğŸ“Œ ì‹¤í–‰ ì˜ˆì œ
if __name__ == "__main__":
    df_data = load_data_from_mssql()  # ğŸ”¥ ìµœì‹  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    train_lightgbm(df_data)  # LightGBM í•™ìŠµ
    test_qr = "123ABC456"  # ì˜ˆì¸¡í•  QR ê°’
    predict_qr(test_qr)  # ì˜ˆì¸¡ ì‹¤í–‰
ğŸ“Œ ì´ ë°©ì‹ì´ ì¢‹ì€ ì´ìœ 
âœ… Boosting ì‚¬ìš© (LightGBM) â†’ ë³µì¡í•œ ë°ì´í„° íŒ¨í„´ì„ ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ í•™ìŠµ
âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ íŠœë‹ (Optuna) â†’ ìµœì ì˜ ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ì°¾ìŒ
âœ… ìë™ ë°ì´í„° ë¶„í•  â†’ í•™ìŠµ / ê²€ì¦ / í…ŒìŠ¤íŠ¸ ë°ì´í„° ê´€ë¦¬ ìµœì í™”
âœ… ê¸°ì¡´ ë°ì´í„° ì „ì²´ë¥¼ ë‹¤ì‹œ í•™ìŠµí•˜ì§€ ì•Šê³ , ìƒˆë¡œìš´ ë°ì´í„°ë§Œ ì¶”ê°€ í•™ìŠµ
âœ… ë©”ëª¨ë¦¬ì™€ ê³„ì‚°ëŸ‰ ì ˆì•½ (ê¸°ì¡´ ëª¨ë¸ì„ ë®ì–´ì“°ì§€ ì•Šê³  ìœ ì§€ ê°€ëŠ¥)

ğŸ“Œ ì´ ë°©ë²•ì´ í˜„ì‹¤ì ìœ¼ë¡œ ê°€ì¥ ì¢‹ì€ ì´ìœ 
ë”¥ëŸ¬ë‹(LSTM)ë³´ë‹¤ LightGBMì´ ë” ë¹ ë¥´ê³ , ì ì€ ë°ì´í„°ë¡œë„ í•™ìŠµ ê°€ëŠ¥
í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ìë™ìœ¼ë¡œ í•´ì„œ ì¶”ê°€ í•™ìŠµ ì‹œ ëª¨ë¸ì´ ìµœì í™”ë¨