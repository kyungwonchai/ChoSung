import pdfplumber
import re
from typing import List, Dict, Any
from collections import defaultdict
import numpy as np
from sklearn.cluster import KMeans

# ==============================================================================
# 데이터 구조 클래스 (출력 형식 변경)
# ==============================================================================
class PartComponent:
    def __init__(self, page_number: int, part_number_str: str):
        self.page_number = page_number
        self.part_number = part_number_str
        # ? 이제 상세 정보는 '라인' 단위의 문자열 리스트로 저장됩니다.
        self.details_lines: List[str] = []

    def add_line(self, line_text: str):
        self.details_lines.append(line_text)

# ==============================================================================
# ? 단어(word) 묶음을 '한 줄'의 텍스트로 재조립하는 함수 ?
# ==============================================================================
def reconstruct_lines_from_block(block_words: List[Dict[str, Any]]) -> List[str]:
    """
    단어 객체 리스트를 받아, 수직 위치를 기반으로 텍스트 라인 리스트를 반환합니다.
    """
    if not block_words:
        return []

    # 단어를 위->아래, 왼쪽->오른쪽 순으로 정렬
    sorted_words = sorted(block_words, key=lambda w: (w['top'], w['x0']))
    
    lines = []
    current_line_words = [sorted_words[0]]
    
    for i in range(1, len(sorted_words)):
        prev_word = current_line_words[-1]
        current_word = sorted_words[i]
        
        # 같은 줄에 있는지 판단 (top 좌표가 거의 같으면 같은 줄로 간주)
        # 허용 오차(tolerance)는 글자 높이의 절반 정도로 설정
        tolerance = prev_word['bottom'] - prev_word['top'] / 2
        if abs(current_word['top'] - prev_word['top']) < tolerance:
            current_line_words.append(current_word)
        else:
            # 이전 라인 완성
            current_line_words.sort(key=lambda w: w['x0']) # 최종적으로 x축 정렬
            lines.append(" ".join(w['text'] for w in current_line_words))
            # 새 라인 시작
            current_line_words = [current_word]
    
    # 마지막 라인 추가
    current_line_words.sort(key=lambda w: w['x0'])
    lines.append(" ".join(w['text'] for w in current_line_words))
    
    return lines

# ==============================================================================
# 메인 분석 함수
# ==============================================================================
def analyze_parts_from_pdf(pdf_path: str, output_txt_path: str):
    all_part_components = []
    part_number_pattern = re.compile(r"^\d{4}-\d{6}$")

    print(f"PDF 분석 시작: {pdf_path}")
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages, 1):
                print(f"\n--- 페이지 {page_num} 처리 시작 ---")
                
                raw_words = page.extract_words(extra_attrs=["size"])
                sanitized_words = [w for w in raw_words if 'top' in w and 'bottom' in w and w['bottom'] > w['top']]

                if len(sanitized_words) < 2: continue
                
                sorted_words = sorted(sanitized_words, key=lambda w: w['top'])

                # 1단계: K-Means로 그룹 분리 기준 학습
                gaps = np.array([sorted_words[i+1]['top'] - sorted_words[i]['bottom'] for i in range(len(sorted_words) - 1) if sorted_words[i+1]['top'] > sorted_words[i]['bottom']]).reshape(-1, 1)
                if len(gaps) < 2: continue
                kmeans = KMeans(n_clusters=2, random_state=0, n_init='auto').fit(gaps)
                center1, center2 = kmeans.cluster_centers_.flatten()
                separation_threshold = gaps[kmeans.labels_ == np.argmax([center1, center2])].min()

                # 2단계: 학습된 기준으로 블록 분리
                blocks, current_block = [], [sorted_words[0]]
                for i in range(len(sorted_words) - 1):
                    gap = sorted_words[i+1]['top'] - sorted_words[i]['bottom']
                    if gap >= separation_threshold:
                        blocks.append(current_block)
                        current_block = [sorted_words[i+1]]
                    else:
                        current_block.append(sorted_words[i+1])
                blocks.append(current_block)
                print(f"페이지 {page_num}에서 {len(blocks)}개의 정보 블록을 감지했습니다.")

                # 3단계: ? 각 블록을 라인 단위로 재조립하고 결과 생성
                for block in blocks:
                    found_part_number_obj = next((w for w in block if part_number_pattern.match(w.get('text', ''))), None)
                    if found_part_number_obj:
                        component = PartComponent(page_num, found_part_number_obj['text'])
                        
                        # 블록 내 단어들을 라인으로 재조립
                        reconstructed_lines = reconstruct_lines_from_block(block)
                        
                        for line in reconstructed_lines:
                            component.add_line(line)
                        all_part_components.append(component)

        # 최종 결과 출력 (라인 단위로)
        with open(output_txt_path, 'w', encoding='utf-8') as f:
            f.write(f"총 {len(all_part_components)}개의 부품을 찾았습니다.\n")
            
            page_map = defaultdict(list)
            for component in all_part_components:
                page_map[component.page_number].append(component)

            for page_num in sorted(page_map.keys()):
                components_on_page = page_map[page_num]
                f.write("\n" + "="*25 + f" PAGE {page_num} " + "="*25 + "\n")
                f.write(f"({len(components_on_page)}개의 부품 발견)\n\n")
                for i, component in enumerate(components_on_page, 1):
                    f.write(f"--- {i}. PartNumber: {component.part_number} ---\n")
                    f.write("Details:\n")
                    # 라인들을 하나씩 출력
                    for line in component.details_lines:
                        f.write(f"\t- {line}\n")
                    f.write("-" * 60 + "\n")

        print(f"\n분석 완료! 결과가 '{output_txt_path}' 파일에 저장되었습니다.")

    except Exception as e:
        print(f"스크립트 실행 중 오류가 발생했습니다: {e}")
        import traceback; traceback.print_exc()

# --- 메인 실행 부분 ---
if __name__ == "__main__":
    # 필수 라이브러리: pip install scikit-learn numpy
    pdf_file_path = "YOUR_PDF_FILE_PATH.pdf" 
    output_file_path = "part_list_final_output.txt"
    analyze_parts_from_pdf(pdf_file_path, output_file_path)